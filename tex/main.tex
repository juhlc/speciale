\documentclass[11pt, a4paper]{memoir}
\usepackage[english, science, dropcaps, hyperref, submissionstatement]{ku-frontpage}
\usepackage{amsmath,amssymb,amsfonts,mathrsfs,latexsym,mathtools}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{ulem}
\usepackage{centernot}

\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Definition}

\newcommand{\mN}{\mathbb{N}}
\newcommand{\mZ}{\mathbb{Z}}
\newcommand{\mQ}{\mathbb{Q}}
\newcommand{\mR}{\mathbb{R}}
\newcommand{\mD}{\mathbb{D}}
\newcommand{\mH}{\mathbb{H}}
\newcommand{\mC}{\mathbb{C}}
\newcommand{\mP}{\mathbb{P}}
\newcommand{\mF}{\mathbb{F}}
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\indep}{\perp \!\!\! \perp}

\setlength\arraycolsep{2 pt}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{0}

\DeclareRobustCommand{\firstsecond}[2]{#2}

\assignment{Master's thesis}

% The following are only needed if the \author, \title, \subtitle, and \date
% commands are not patchable. See the readme for more information.
% \frontpageauthor{Alex Author}
% \frontpagetitle{A concise but nevertheless\\precise and interesting title}
% \frontpagesubtitle{An intruiging subtitle}
% \frontpagedate{Submitted: \today}

\frontpagetitle{Causal Inference in Dynamic Systems}
\subtitle{Convergent Cross Mapping and Alternative Approaches}
\frontpageauthor{Rasmus Juhl Christensen}
\frontpagedate{Submitted: \today}
\advisor{Advisor: Niels Richard Hansen}
\frontpageimage{example.png}

\kupdfsetup{A concise but nevertheless precise and interesting title - An intruiging subtitle}{}{Alex Author}

\begin{document}
\begingroup
  \fontencoding{T1}\fontfamily{LinuxLibertineT-OsF}\selectfont
  \maketitle
\endgroup

\section{Abstract}

\subsubsection{Description from contract}
The main goal of the project is to investigate dynamic systems for time series and the effectiveness of convergent cross mapping (CCM) to detect causality in this framework compared to other approaches on the basis of a reference model. Drawing on the paper "Detecting Causality in Complex Ecosystems" (2012) and "Distinguishing time-delayed causal interactions using convergent cross mapping", the project will explore the theoretical foundations, practical applications and potential shortfalls of CCM and its link to more classical methods in causality.\\\\
Reference 1: Detecting Causality in Complex Ecosystems. George Sugihara, Robert May, Hao Ye, Chih-hao Hsieh, Ethan Deyle, Michael Fogarty and Stephan Munch. Science , 26 October 2012, New Series, Vol. 338, No. 6106 (26 October 2012), pp. 496-500.
Reference 2: Distinguishing time-delayed causal interactions using convergent cross mapping. Hao Ye, Ethan R. Deyle, Luis J. Gilarranz \& George Sugihara. Scientific Reports volume 5, Article number 14750, 2015.\\\\
 
\textit{Front page image generated by Chaoscope.}

\newpage

\tableofcontents


\vfill
\noindent All code and data used in this project is available at \url{github.com/juhlc/speciale}.

\normalem

\newpage

\section{Introduction}
This thesis will tackle the conceptually challenging topic of causal inference. In traditional statistics, we distinguish between correlation and causation: Correlation is a feature of covariation between variables of interest, whereby one observes a linear relationship between changes in two variables and having observed a deviation in one variable makes a deviation more likely in the other. As such, this can be leveraged for prediction - you can use the one variable to better predict the other. If one has control of the one variable, say the treatment of a patient, one can randomize the treatment to ensure that if patients under treatment fare better than patients without treatment, then this can only be explained by the treatment. This allows us to infer causation, which is the feature of covariation between variables that allows one to assert that one variable causes another, which means that if you force a change in the distribution of the causing variable, you will also enforce a change in the effect variable. This is straightforward with randomization, but if you can not control your variable and only observe, then an interpretation of causation may be more tricky to defend. We will explore a framework that can allow us to infer causal relationships in dynamic systems.

\subsection{Causal modelling in time series}
In this section, we will introduce and compare definitions of causal links and we will try to illuminate the definitions and motivations of each. As mentioned before, we interpret causality as the notion that intervening in the causing variable effects an intervention in the effect variable. This can be interpreted as a distributional shift in a probabilistic setting, but it does not preclude us from using the same intuition in deterministic settings. The science of identifying such links from observational data is that of causal inference and one approach to formalization is that of the Structural Causal Model (SCM), which in its most simple form can be formulated for two variables as
\begin{align*}
C&\coloneqq N_C\\
E&\coloneqq f(C,N_E)
\end{align*}
where $N_C,N_E$ are independent random variables of some distribution and $f$ a function. In this case, we denote $C$ the cause and $E$ the effect. This simple model has some deficiencies, one being that if $N_E$ is degenerate and $f$ invertible (meaning that $x\to f(x,N_E)$ is invertible), then $E$ is the cause in the SCM
\begin{align*}
E&\coloneqq N_E\\
C&\coloneqq f_x^{-1}(E,N_C)
\end{align*}
where $N_C$ is degenerate. This prevents us from having any chance of identifying the direction of a causal link and perhaps meaningfully discussing the concept of causality. In this case, we have a deterministic relationship between our variables, and so without further structure on the problem it is perhaps most natural to consider it impossible to detect causal links in this setting. This leads us to the type of problems we consider in our setting: we introduce the dependence upon time, that is we consider a series of connected data. First of all, the most pressing problem is that of incorporating the aspect of temporal dependence in the structual causal model. The definition of the SCM easily extends to any number of variables with the definition that $X$ causes $Y$ if $Y\coloneqq f(X,Z_1,...,Z_n,N_X)$ and for a moment disregarding that $f$ may trivially depend on any variable - say in this context that $f$ depends 'non-trivially' on $X$. We can then easily imagine a a time series in an SCM in a discrete framework by simply associating each variable at a given time with a unique variable. Imposing that a variable at any given time can only cause other variables further forward in time gives this model an appropriate time structure. We inherit the problems with causality in pathological deterministic structures, and it is not immediate that this model is appropriate for continuous time models as well. However, the upside of this model is that it allows for an alternative formulation of cause and effect under some admittedly rather restrictive assumptions, namely Granger causality:\\
\textit{Consider an SCM without instantaneous effects for the time series $X_t$. Then $X^j$ causes $X^k$ if and only if there exists $t'\in \mZ$ such that
$$X_{t'}^j\centernot \indep X_{t'}^k\mid X_{t<t'}^{-j}$$}
Informally, this implies that $X^j$ causes $X^k$ if the prediction of $X^k$ based on all available information is improved when including $X^j$. In principle, this would require one to observe and include all relevant variables in the world.\\
Alternatively, we can approach the problem from a different angle. Granger causality approaches the problem from a probabilistic setting, and is thus irrelevant in deterministic settings. Furthermore, when implementing Granger causality in practice, basing causal inference upon the improvement in prediction when including a variable is heavily compromised in dynamic systems where the variables are governed by an underlying structure whereby information about other variables will be encoded in a variable simply by the nature of the dynamic system - and as such implementations of Granger causality may erroneously attribute causal links between variables that are not causally linked. In fact, in dynamic systems we have the result of Takens (1981) suggesting that observed values of one variable from a dynamic system may in fact suffice in obtaining the dynamics of the entire systems. We will discuss the theoretical details hereof later. Meanwhile we can exploit this idea of information being encoded in effect variables to great effect: this allows us to reformulate causality from the point of view of deterministic systems. The principle itself is fairly simple, but we do require some technical assumptions to make the theory work, which are not in any way empirically motivated nor entirely interpretable, but fare under what we usually consider regularity assumptions - a sweep-it-under-the-rug-interpretation would be that we just assume the data to be reasonably well-behaved. The basic idea is that if $C$ is the cause of $E$, then $E$ must contain information of $C$, since the dynamics of $C$ must be reflected in $E$ in light of a functional relationship, whereas the reverse cannot be true. Thus when reconstructing the entire dynamic system from the observed values of $E$ in accordance with the theorem, we would expect to recover the dynamics of $C$. We should however expect the reverse statement to be false, since the self-contained dynamics of $E$ would not be reflected in $C$, and so the dynamic system reconstructed from the observed values of $C$ would not recover the dynamics of $E$. Notice that this in fact implies the opposite of Granger causality: we should be able to predict the cause $C$ using the effect $E$ but not conversely. This is quite a radical difference. To obtain further understanding, we will have to delve into the theoretical details. 

\chapter{Causal Inference with State Space Reconstruction}
In this chapter, we will give an introduction to an approach to causal inference in dynamic systems suggested by \cite{Sugihara}. Dynamic systems will in this thesis be used to describe the development of a set of variables of interest over time with the assumption of some deterministic non-linear relationship between the development of the variables. In the paper of \cite{Sugihara}, they introduce an approach called \textit{Convergent Cross Mapping}, which builds on the theory of state space reconstruction, which is formulated in the terminology of differential topology. This approach is inspired by Takens' theorem of 1981, \cite{Takens}, that solves the problem of constructing embeddings into $\mR^n$ of higher-dimensional systems with only a single variable in the specific case when we are working with compact manifolds. We will explain this theory in more detail in this chapter.\\\\
First, we introduce the problem of causal inference in complex dynamic systems with the example used by \cite{Sugihara} regarding the interactions between anchovies, sardines and sea surface temperature in the California Current ecosystem as a motivation for the more theoretical nature of the rest of this chapter. In the following, we will give a brief introduction to the causal framework of structural causal models inspired by the account of \cite{Peters}, which will serve both as a means of introducing a language for causal inference that we can apply in the rest of the thesis, but also to use as a guideline for formulating a consistent framework that allows for causal inference in a deterministic setting as well as in the usual stochastic setting. Since these settings are somewhat incomparable due to their roots in different mathematical fields, this will also be the basis of an attempt to relate the assumptions natural in the different settings to the standard of equivalency. This inevitably will produce some conceptual problems as the notion of causality is philosophical in nature, thus endowing any attempt to give mathematical certainty to the process of causal inference some complications and notable exceptions worth discussing. We will briefly discuss these problems and try to be explicit about the assumptions and potential problems of the causal framework chosen in this thesis. We will also briefly discuss alternative formulations less conducive to overly strong interpretations.\\\\
We then turn to a more stringent introduction to the theory of embeddings allowing us to formulate the main results in full detail. This will prove very useful for giving explicit assumptions and deriving explicit results for identifiability of causal links later on and therefore we spend some time in the theoretic world of differential topology examining the definitions and proof techniques on an introductory level. We will also discuss some expansions and alternative formulations to the main theorem of \cite{Takens}. This lays the foundation for the main theorem of the thesis, namely a complete characterization of the causal inference possible from the approach suggested by \cite{Sugihara}, which builds on the work of \cite{mathFound}. This will complete the introduction of the theoretical set-up of the central model of this thesis.\\\\
We end this chapter with a detailed account of the concrete approach suggested by \cite{Sugihara}, and we then discuss alternatives and extensions hereof. Notably, we will discuss the extensions of the approach suggested by \cite{Ye2015} to try to resolve the problem of distinguishing bidirectional causality and unidirectional causality in systems with some degree of synchronicity; and the extension suggested by \cite{Leng2020} to try to identify the causal chain in a system where we know the causal direction between causally related variables. These problems cannot be resolved by the approach of \cite{Sugihara} as we shall see shortly. We will then return to our motivational example from the beginning and provide an analysis hereof using the theory developed. This is all done with the caveat that the theory developed in this setting applies only to purely deterministic systems and a more appropriate analysis of the example and in reality any complex ecosystem or more generally an observed dynamical system would include some deterministic dynamic structure and some stochastic structure. We return to this problem in later chapters: we will try to extend our considerations here to stochastic dynamic systems in chapter ? replacing systems of differential equations with systems of stochastic differential equations, and we will do a simulation based analysis of the effect of measurement noise or of stochastic influences on the effectiveness of our approach in chapter ?.

\section{Sardines and Anchovies in the California Current System}
In this section, we will briefly explore the relationship between Pacific sardine (\textit{Sardinops sagax}) landings in pounds, Northern anchovy (\textit{Engraulis mordax}) landings in pounds, and sea-surface temperature (SST) measured at Scripps Pier as collected by \cite{Scripps} and at Newport Pier as collected by \cite{Newport} all recorded every month. We will use the same data as \cite{Sugihara}, and we obtain the SST estimates from the University of California San Diego, we obtain older fishing data from \textit{ERDDAP} at \cite{oldData} and newer data directly from \cite{newData}. We have data from 1928 until 2022, and we analyze a 4-year running average of our variables as to avoid all seasonal effects on the data and particular weather incidents in a given year. Below we plot the yearly landings and a smoothed 4-year average of the monthy landings for sardine and anchovy against a 4-year average of the sea surface temperature:
\begin{center}
	\includegraphics[scale=0.79]{{"../r code/sardine"}.png}
\end{center}
There is a lot of redacted data to consider in these data sets, as pursuant to California Fish and Game Code Section 8022, commercial landings data is considered confidential. Therefore some landings data is missing due to insufficient data to summarize and maintain confidentiality \url{https://wildlife.ca.gov/Conservation/Marine/Data-Management-Research/MFDE/User-Guide}. Since the particular question in hand is not of significant interest for the thesis itself and only intended to be an illustration of the systems we intend to study, we will set all missing data to zero. For a more comprehensive study of sardine and anchovy landings, we refer to . What strikes the eye about this graph is the apparent negative correlation between sardine and anchovy landings in most of the 20th century. This phenomenon was not exclusive to the Califonia Current System, and this lead to the hypothesis of internal competition between the two species begin a driving factor in the dynamics of the system \textbf{cite}. In particular, the Sardine ecosystem experienced an almost total ecological collapse in the 1970's, here portrayed by the insignificant landings of sardine in this period, and competing hypotheses as to the reason of this collapse was presented and continued all through the 1990's - we refer to ... for a comprehensive study of the interdynamics of sardine and anchovy landings. In some places, the theory of interspecies competition led to policies intensifying the fishing of anchovies in the hope that this would benefit the sardine ecosystem \textbf{cite}, although this approach turned out to be unfruitful.\\\\
When approaching the 21st century, we observe however a positive correlation between sardine and anchovy landings thus illustrating the complex dynamics of the system and perhaps the absence of a causal relationsship. This is also a reminder of the well-known fact that correlation does not imply causation. Several papers suggest that the temperature is in fact the driver of the system, and the analysis of \cite{Sugihara} confirms this result. Below we plot the correlation patterns of the system, by plotting the correlation between our variables based on the surrounding 25 years:

\section{Causal Framework}
We will first give an introduction to our causal modelling in the terminology of the structural causal model (SCM), sometimes also referred to as a structural equation model (SEM),  which allows us to model time series in a discrete framework. We shall then discuss the extension to continuous time series and the link to the discrete time series framework. Our causal framework does come with some caveats, but we will try our best to discuss them along the way. We will present the SCMs as done in \cite{Peters}, and we will freely use standard terminology from graph theory. We start with the central object of study.
\begin{defn}[Discrete time series]
We shall call $(X_t)_{t\in \mZ}=(X_t^{1},\ldots,X_{t}^d)_{t\in \mZ}$ a \emph{$d$-dimensional discrete time series}, where $X_t^j:\Omega\to \mR$ is a random variable and the observation of the $j$th variable of $X$ at time $t$.
\end{defn}
\begin{defn}[Causal graph]
Let $\mathcal{V}$ be a set consisting of vertices representing each observation of $X$, $X_t^j$, say $$\mathcal{V}=\left\{X_t^j: t\in \mZ,j\in\{1,\ldots,d\}\right\}\simeq \left\{(t,j)\mid t\in \mZ, j\in \{1,\ldots,d\}\right\}$$ and let $\mathcal{E}$ be a set of directed edges in the graph, that is 
$$\mathcal{E}\subset \mathcal{V}\times \mathcal{V}$$
satisfying that $(X_{t_1}^{j_1},X_{t_2}^{j_2})\in \mathcal{E}$ implies $t_1\leq t_2$, i.e. that there are no links backwards in time. If $t_1<t_2$ for all $(X_{t_1}^{j_1},X_{t_2}^{j_2})\in \mathcal{E}$, we shall say $\mathcal{E}$ contains no \emph{instantaneous effects}.\\
We shall call $(\mathcal{V}, \mathcal{E})$ a \emph{causal graph} of the time series $(X_t)_{t\in \mZ}$. If there exists $v_1,v_2\in \mathcal{V}$, such that $(v_1,v_2)\in \mathcal{E}$, we shall say $v_1$ is a \emph{parent} of $v_2$, write $v_1\in \text{PA}_{v_2}$, and if $v_2=X_t^j$, we shall write $\text{PA}_{t}^j$ for $\text{PA}_{v_2}$. If $X_{t_1}^{j_1}$ is the parent of $X_{t_2}^{j_2}$, we shall say $X_{t_1}^{j_1}$ \emph{directly causes} $X_{t_2}^{j_2}$ and denote $X_{t_1}^{j_1}$ the \emph{direct cause} and $X_{t_2}^{j_2}$ the \emph{direct effect}.
\end{defn}
In much of the causality literature, we would impose that the graph be acyclic, making the causal graph a directed acyclic graph (DAG). We will however concern ourselves with bidirectional causality later on, so we will not make this assumption the standard in this thesis. Remark however, that this assumption is fulfilled under any circumstance if we do not allow for instantaneous effects. With the approach of the structural causal model, we put the causal graph front and center in this modelling of causal structures.
\begin{defn}[Structural causal model]
Let $(X_t)_{t\in \mZ}$ be a discrete time series, let $\mathcal{G}=(\mathcal{V},\mathcal{E})$ be a causal graph, let $(N_v)_{v\in \mathcal{V}}$ be a family of random variables (noise variables), and let $f_1,\ldots,f_d$ be a family of deterministic functions that is not constant in any of its arguments. Then we call this setup a \emph{structural causal model}, if
$$X_t^j=f_j(\text{PA}_t^j,U_{X_{t}^j})$$
A few further assumptions are usually added here. Often, we assume the noise variables $(N_v)_{v\in \mathcal{V}}$ to satisfy that the sets $(N_{X_t^1},\ldots,N_{X_t^{d}})$ for values of $t\in \mZ$ are jointly independent. Another typical assumption is $(X_t)_{t\in \mZ}$ being strictly stationary. And finally, often we assume that $\text{PA}_t^j$ contain only observations from time period $t-q$ and forward for a fixed $q\in \mN$.\\
If for some set $A\subset V$, we replace the causal graph $G$ by a causal graph $G'$ that satisfies $\text{PA}_t^j$ is replaced by $J(j)\subset V$ for $j\in A$ in the new causal graph, and if we replace $f_j$ with some $\zeta_j$ for $j\in A$, we call the resulting new structural causal model for the \textit{postintervention structural causal model}.
\end{defn}
Mathematically, there is really nothing preventing us from defining causal graphs for continuous time series, but the interpretability of such a graph is somewhat compromised insofar that we may no longer meaningfully draw it without reverting to some discretization scheme. More problematic, is perhaps how to restrict the assignments in a meaningful fashion.
\begin{defn}[Markov property and faithfulness]
We define graph independence as following
\end{defn}
The following idea was presented by Granger (reference?), and we may actually give explicit conditions for the efficacy of the Granger causality framework in our current model set-up.
\begin{thm}[Granger causality]
O
\end{thm}
We now turn our attention towards continuous time models and we will present the models of driving systems as presented in \cite{mathFound}. The definition of continuous time series is straightforward a generalization of that of discrete time series.
\begin{defn}[Continuous time series]
We shall call $(X_t)_{t\in \mR}=(X_t^{1},\ldots,X_{t}^d)_{t\in \mR}$ a \emph{$d$-dimensional continuous time series}, where $X_t^j:\Omega\to \mR$ is a random variable and the observation of the $j$th variable of $X$ at time $t$.
\end{defn}
More interestingly of course is that of the dynamical systems that we will treat in this thesis. We will take it directly from \cite{mathFound}.
\begin{defn}[Dynamical system]
A \emph{dynamical system} is a collection of maps $\varphi^t:\mR^n\to\mR^n$ indexed by some $t\in \mZ$, in which case we say it is \emph{discrete}, or some $t\in \mR$, in which case we say it is \emph{continuous}.\\
We shall call a function $y:\mR^n\to \mR$ an \emph{observation function}, and we shall note that $y(\varphi^t)_t$ defines a time series.
\end{defn}
As discussed above, considering a graph on the entirety of a continuous time series.
\begin{defn}[Driver]
Given a dynamical system $\varphi$, and two variables $x$ and $y$ of $\varphi$, the variable $x$ \emph{directly drives} $y$ if the dynamics of $y$ directly depend on $x$, that is $y_{t+1}=g(x,\cdot)$ for discrete systems and $\dot{y}=g(x,\cdot)$ for continuous.
\end{defn}
\begin{defn}[Summary graph]
Given a dynamical system $\varphi$, the \emph{interaction graph} or \emph{summary graph} is a directed graph $\mathcal{IG}_\varphi=(\mathcal{V},\mathcal{E})$, with $\mathcal{V}$ corresponding to the variables of $\varphi$ and a directed edge from $x$ to $y$ if $x$ directly drives $y$.
\end{defn}
In some sense, we can think of this graph as the graph obtained by collapsing the time variable in the causal graph; if at some time point $x$ causes $y$, then we put a directed edge from $x$ to $y$ in the interaction graph. Thus we obtain a new graph containing causal directions: remark that assuming this graph is acyclic becomes more untenable.
\begin{defn}[Component graph]
A subset of vertices $W\subset V$ of the interaction graph is called a \emph{strongly connected component}, if for any $u,v\in W$ there exists a directed path from $u$ to $v$ and a directed path from $v$ to $u$. We define the \emph{component graph} as the graph obtained from the interaction graph by taking the set of strongly connected components $\mathcal{S}$ as the set of vertices, and letting there be a directed edge from $p\in \mathcal{S}$ to $q\in \mathcal{S}$ if there exists a $u\in p,v\in q$ such that there exists a directed path from $u$ to $v$.
\end{defn}
Note that being strongly connected defines an equivalence relation. The component graph will turn out to be very relevant in the following analysis.
\begin{defn}[Upper set]
An \emph{upper set} is a subset $U\subset \mathcal{S}$ satisfying that if $q\in U$ and there is a directed path from $p$ to $q$, then $p\in U$.
\end{defn}
It will turn out that each upper set corresponds to a self-contained dynamical system. The final component in our later analysis is the so-called \textit{transitive closure}:
\begin{defn}[Transitive closure]
Given a DAG $\mathcal{G}=(\mathcal{V},\mathcal{E})$, the \emph{transitive  closure of $\mathcal{G}$} is the graph obtained from $\mathcal{G}$ by taking the vertices $\mathcal{V}$ and include the edge between $u\in \mathcal{V}$ and $v\in \mathcal{V}$ if and only if there is a directed path from $u$ to $v$.
\end{defn}
The transitive closure of a causal graph encodes only the causal direction between variables but cannot distinguish between direct and indirect causal links. In other words, from the transitive closure you can infer all descendants of a given variable, but not the causal graph itself. It will turn out that this is exactly what we will be able to recover with the modelling framework based on Takens' theorem.

Now for the link between discrete and continuous time models, we will briefly present the result proved by \cite{sokol2014}, that shows that the discretization of an SDE according to the Euler scheme and treating it with the discrete time model framework will give the same results in the limit.
\section{Takens' Theorem}
In the following, we will switch gears from the more welcoming realm of considering functions in $\mR^n$ and $\mR$ and move to the realm of differential topology, which is inhabited by manifolds, vector fields and tangent bundles and worse. This is done, not for needless abstraction, but because the theory we shall be exploiting holds in this generality - and in fact as long we consider compact manifolds, we might as well think of them in Euclidean spaces nonetheless. So in practice, we will always be able to envision our models living in our usual space $\mR^n$, but it is not in any way useful to restrict ourselves at this point.\\
We will consider different setups, but the base model is systems modelled as vector fields on invariant compact manifolds (separable, Hausdorff, locally Euclidean spaces), which can be generalized to cover vector fields with an attractor set, that need not in fact be a manifold, see \cite{Sauer1991}. As the theorem itself, this is formulated in the realm of differential topology and so the objects involved in the theorem and subsequqnt theory spring from there. Below we will briefly introduce and recite the definitions of the objects of relevance, if nothing else to clarify the terminology used in this thesis. In practice, most of our examples will come from partial differential equations (PDEs) and stochastic differential equations (SDEs); we will consider the link between these realms in more detail later. We will for now interpret the manifold as the 'path' of the system, that is the values that the system takes throughout time. With terminology borrowed from physics, one could call this the \textit{phase space}. The fundamental result we shall use, now known as Takens' theorem, was proved in 1981 by Floris Takens, \cite{Takens}, and states that one can reconstruct the manifold using only a univariate observation function; that is observing only one variable over time allows us to reconstruct the entire path of the dynamic system of all variables. The original theorem as stated by Takens is as follows:
\begin{thm}[Takens, Version 1]
Let M be a compact manifold of dimension $m$. For pairs $(\varphi,y)\in \text{Diff}^2(M)\times C^2(M,\mathbb{R})$, it is a generic property that the map $\Phi_{(\varphi,y)}:M\to \mathbb{R}^{2m+1}$, defined by
$$\Phi_{(\varphi,y)}(x)=(y(x),y(\varphi(x)),...,y(\varphi^{2m}(x))),$$
is an embedding. \cite{Takens}
\end{thm}
There are several things to unpack here. First of all, it is worth noting the differentiability constraints on our maps; both $\varphi: M\to M$ and $y:M\to\mR$ are required to be twice continuously differentiable. In the work of \cite{Sauer1991}, their setup enables this requirement to be relaxed to them only being once continuously differentiable. Our dynamical system is indeed a vector field on an invariant compact manifold being the first regularity condition, this is then the second regularity condition, that we will for the most part silently have to assume is fulfilled. Next, we will call $y$ our \textit{observation function}, and we can think of this as a series of observations made from the system. For now, $\varphi$ remains a bit more mysterious, but we shall quickly make this map must more explicit. We shall call $\Phi$ for the \textit{delay embedding}, the reasoning behind which will be revealed shortly. However, the most important caveat is perhaps that it is 'only' a generic property that the theorem yields an embedding. We will provide more rigorous definitions later, but for now we will understand embedding simply as a reconstruction of the manifold. By generic property is meant to be understood that for 'most' observation functions $y$ and most choices $\varphi$, the delay embedding is indeed an embedding. The precise meaning in an algebraic sense is that there exists an open and dense subset of the function space $C^2(M,\mR)$, respectively $\text{Diff}^2$, for which the statement is true. From this stems our second regularity assumption. For $\varphi$, we will be able to do some reasoning in practical cases, but for $y$ this will often be just another silent assumption. We shall be calling a map $y\in C^2(M,\mR)$, respectively $\varphi\in \text{Diff}^2(M)$, \textit{generic} if there exists $\varphi\in \text{Diff}^2(M)$, respectively $y\in C^2(M,\mR)$, such that the resulting delay embedding is indeed an embedding.\\\\
We will now work towards a more explicit version of the theorem. First of all, the reasoning behind the naming of the delay embedding, comes from the following: we shall associate any given time point $t\in \mR_{\geq 0}$ with a position $x(t)\in M$ on the manifold. For a given delay length $\tau\in \mR_{>0}$, we may then define $\varphi_\tau: M\to M$ by $x(t)\mapsto x(t-\tau)$. Substituting this into Takens' theorem reduces the delay embedding to
$$\Phi(x(t))=(y(x(t)),y(x(t-\tau)),\ldots,y(x(t-2m\tau)))$$
Takens' theorem thus states that the values of a single time series considered at different delays will reconstruct the manifold 'generically'. Now we will need to ensure that this choice of $\varphi$ is both twice continuously differentiable and is not of such a nature that it falls out of the set of generic functions. The differentiability constraint falls under the umbrella of regularity assumptions, while we can guarantee that $\varphi_\tau$ will be generic for some choices of $\tau$ by the power of a restatement of Takens' theorem. It was in fact proved by Takens in \cite{Takens}, but not stated. We will follow the restatement of the theorem as well as the subsequent presentation of theory and definitions presented in \cite{Huke} for the rest of this section.
\begin{thm}[Takens, version 2]
Let M be a compact manifold of dimension $m$. Let $\varphi\in\text{Diff}^2(M)$ satisfy
\begin{enumerate}[label=\arabic*)]
	\item the periodic points of $\varphi$ with periods less than or equal to $2m$ are finite in number,
	\item if $x$ is any such periodic point with period $k\leq 2m$, then the eigenvalues of $\varphi^k$ at $x$ are all distinct.
\end{enumerate}
 Then it is a generic property for $y\in C^2(M,\mR)$ that the map $\Phi_{(\varphi,y)}:M\to \mathbb{R}^{2m+1}$, defined by
$$\Phi_{(\varphi,y)}(x)=(y(x),y(\varphi(x)),...,y(\varphi^{2m}(x))),$$
is an embedding. \cite{Huke}
\end{thm}
By periodic with period less than or equal to $2m$, we mean that $\varphi^{k}(x)=x$ for some $k\leq 2m$. If $\tau$ is chosen such that there are no periodic points of $\varphi_\tau$, or if we assume some structure on the periodic points, then we need only worry about whether $y$ is generic. In fact, if $\varphi_\tau$ is not generic for some choice of $\tau$, it will usually be after some small perturbation in $\tau$ (\textit{is it true, reference?}). Therefore in practice multiple lags can be tested, and we should thus be able to fairly easily sidestep this problem. Before moving on to definitions and technicalities, we will at this point however notice that we have a theorem that gives an explicit construction that will fairly generally allow us to completely recover the dynamics of a system from only a single variable observed over time. In contrast to the choice of $\tau$, we will however usually have in practice that the observation functions are fixed. So even if the same technique of small perturbations of a non-generic observation function produces a generic observation, we do not have access to it. Therefore we will usually have to assume a given observation function is generic, if there is no reason to believe otherwise. However, in some cases we can actually endow genericity with causal interpretation - we return to this in the next section. For now, we expand upon Takens' theorem.  We start of with the basic definitions needed following very closely the terminology and presentation from \cite{Huke2}. 
\begin{defn}[Basics of differential topology]
Let $M$ be a separable Hausdorff space and let $m\in \mN$.\\
- We call $(U,h)$ a \emph{chart} if $U\subset M$ is open and $h: U\to \mR^m$ is a homeomorphism onto its range with $U$ the \emph{chart domain} and $h$ the \emph{coordinate function}.\\
- If for each point $x\in M$, there exists a chart $(U,h)$ on $M$ such that $x\in U$, we call $M$ a \emph{manifold of dimension $m$},\\
- and we call a collection of charts whose chart domains cover all of $M$ for an \emph{atlas}.\\
- The collection of all charts on $M$ is itself an atlas that we call the \emph{structure} on $M$.\\
- On overlapping chart domains of charts $(U,h)$ and $(V,g)$, we consider the \emph{coordinate transformations}
$$hg^{-1}: g(U\cap V)\to \mR^m, \text{ and } gh^{-1}: h(U\cap V)\to \mR^m$$
- We say the charts are \emph{$C^r$-related} if the coordinate transformations $hg^{-1}, gh^{-1}$ are $C^r$,\\
- and if all coordinate transformations of an atlas are $C^r$-related, we say the atlas is \emph{$C^r$-differentiable}.\\
- A \emph{differential structure} is the set of all charts $C^r$-related to the charts in some atlas.\\
- We shall say a manifold is $C^r$ if there exists a $C^r$-differentiable atlas of $M$.\\
- If $M$ is $C^r$ and $N$ is a $C^r$ manifold of dimension $n\geq m$, the function $f: M\to N$ is \emph{$C^r$-differentiable} if for each $p\in M$, there exists charts $(U,h)$ on M and $(V,g)$ on N, such that $p\in U$, $f(p)\in V$ and 
$$gfh^{-1}: h(U\cap f^{-1}(V))\to \mR^n$$
is $C^r$ (remark: this property thus holds for any choice of charts in the atlas covering $p$, $f(p)$ respectively).\\
- The \emph{Jacobian} at $p$ is then defined as 
$$Dgfh^{-1}(h(p))$$
This depends on the choice of charts but the rank does not.\\ 
- If the Jacobian is of maximal rank (rank $m$), we say $f$ is \emph{immersive at p},\\
- and if $f$ is immersive everywhere, then we say $f$ is an \emph{immersion}.\\
- An immersion that is homeomorphic upon its image is an \emph{embedding}.\\
- Conversely, if $m\geq n$, and the Jacobian is of maximal rank (rank $n$), then $f$ is \emph{submersive at $p$},\\
- and if submersive everhywhere a \emph{submersion}.\\
- If $f: M\to N$ is an embedding, we say that $f(M)$ is a \emph{submanifold} of $N$.\\
- A \emph{diffeomorphism} is a map $f:M\to N$ for which there exists a differentiable inverse, and in this case we shall call $M$ and $N$ \emph{diffeomorphic}. If $f$ is an embedding, it is possible to prove that $f:M\to f(M)$ is a diffeomorphism.\\
- Finally, we will define some topologies. We let $C^r(M,N)$ denote the space of all $C^r$ differentiable maps from $M$ to $N$. We then let the topology of $C^r(M,N)$ be generated by the subbase consisting of sets defined as follows: for any choice of function $f\in C^r(M,N)$, charts $(U,h)$ on $M$ and $(V,g)$ on $N$, a compact set $K\subset U$ such that $f(K)\subset V$ and $\varepsilon>0$, we define $\mathcal{N}(f,(U,h),(V,g),K,\varepsilon)$ to be the set of all functions $\hat{f}\in C^r(M,N)$ for which $\hat{f}(K)\subset V$ and
$$\norm{D^kg\hat{f}h^{-1}(x)-D^kgfh^{-1}(x)}<\varepsilon$$
for all $x\in h(K)$, $k=0,\ldots,r$, where $\norm{\cdot}$ is the usual Euclidean norm. If $M$ and $N$ are diffeomorphic, we let $\text{Diff}^r(M,N)$ be the subspace of $C^r(M,N)$ consisting of $C^r$-differentiable diffeomorphisms equipped with the subspace topology. Usually, we will use the shorter $\text{Diff}^r(M)$ for $\text{Diff}^r(M,M)$.
\end{defn}

Before moving on, we will outline the main stages in Takens' theorem just to get an understanding of the proof techniques. In the following, we will use the subspace topology on $C^2(M,\mR)$ inherited from $C^1(M,\mR)$ and likewise on $\text{Diff}^2(M)$. In all topological statements below, this is what is meant. We proceed by proving the 2nd version of Takens' theorem as follows:
\begin{enumerate}[label=\roman*)]
	\item The first stage hangs on using the following standard fact in differential topology:
	\begin{thm}
	Let $M$ be a compact manifold, and let $N$ be any manifold. The set of $C^r$ embeddings of $M$ in $N$ is open in $C^r(M,N)$. \cite{hirsch}
	\end{thm}
	Proving that the map $\mathcal{F}^2: C^2(M,\mR)\to C^2(M,\mR^{2m+1})$, defined by $$y\mapsto (y,y\circ\varphi,\ldots, y\circ \varphi^{2m}),$$ 
	is continuous then immediately proves that the set of generic observation functions is open in $C^2(M,\mR)$. This is done in the following familiar set of steps: one proves $F: C^2(M,\mR)\to C^2(M,\mR)$ defined by $y\mapsto y\circ\varphi$, for $\varphi\in \text{Diff}^2(M)$, is in fact continuous.  This is simply a matter of unravelling the definition of the topology above, and requires only a slight bit of trickery. We skip the details of these derivations. By induction, it follows that $F_n:  C^2(M,\mR)\to C^2(M,\mR)$ defined by $y\mapsto y\circ\varphi^n$ is in fact continuous.  The final detail is to use that the product topology of $C^r(M,\mR)^{2m+1}$ coincides with that of $C^r(M,\mR^{2m+1})$. Worth remarking here is that we have made no assumptions on $\varphi$ yet, any embedding will do. Likewise, we have not used $m$ to do anything yet. 
\end{enumerate}
The following technical fact will prove fairly useful - and it is just a matter of exploiting the compactness of $M$ and using the definition of the topology. If $y\in C^2(M,\mR)$, and $\psi_i\in C^2(M,\mR)$, $i=1,\ldots,N$ for some $N\in \mN$. Then for every neighbourhood $\mathcal{N}$ of $y$, there exists $\delta$ such that
$$y+\sum_{i=1}^N a_i\psi_i\in \mathcal{N}$$
for all $a=(a_1,\ldots,a_n)$ with $\norm{a}< \delta$. 
\begin{enumerate}[label=\roman*)]
	\setcounter{enumi}{1}
	\item The following stages all revolve around exploiting this fact. To prove that the set of generic functions are dense, we consider an arbitrary $y\in C^2(M,\mR)$ and a neighbourhood $\mathcal{N}_y$ of $y$. Now we want to identify a generic observation function in $\mathcal{N}_y$. First, we return to the conditions of the 2nd version of Takens' theorem. Let us denote the points with period less than or equal to $2m$ by $P_{2m}$. Since $|P_{2m}|<\infty$, we know by the Hausdorff property of $M$, that we can find pairwise disjoint neighbourhoods of the elements in $P_{2m}$, and so by a fairly standard argument we can separate these points by observation functions, and we use the fact above. Now we have $y'\in\mathcal{N}_y$ that is injective on $P_{2m}$. Similarly, by doing some explicit calculations on the Jacobian, we can identify a function $y''$ not only injective but also immersive on $P_{2m}$. To be continued...
	\item The proof of the 1st version of Takens' theorem from the 2nd version consists of utilising the Kupka-Smale theorem, namely that it is a generic property that for $\varphi\in \text{Diff}^2(M)$ and some integer $n\in \mN$, the number of periodic points, with period $n$ or less, is finite. There is a little in work in showing that this extends to cover the second condition of the 2nd version, and one also needs to use an argument similar to that in stage i) to prove the openness claim in $\text{Diff}^2(M)\times C^2(M,\mR)$ instead of just in $C^2(M,\mR)$. But the main component is the Kupka-Smale theorem.
\end{enumerate}  
Takens' theorem will allow us to do causal inference in the modelling framework introduced in this chapter. Below we give the theoretical details of this procedure.
\section{Causal Inference with Takens' Theorem}
In this section, we will show that we can only hope to recover the component graph from data using Takens' theorem. We shall later discuss attempts to reason further about the causal structure, but the results below will show that without further assumptions, we cannot distinguish direct and indirect causal links.\\\\
The aim of this section is to show that under suitable regularity assumptions, we can create a correspondence between strongly connected components and sub-manifolds, and that this correspondence is precisely fine-grained enough to allow us to determine whether any two variables belong to the same strongly connected component and further the direction of the causal link if they do not. To do this, we have to formalize this correspondence. First of all, as discussed the component graph $CG_\varphi$ is a directed acyclic graph, and so we can think of the graph structure as a partial order, where $x,y\in CG_\varphi$ satisfy $y\geq x$ if $y\to x$. Considering then the upper sets, we inherit an ordering of these, since they are exactly defined with respect to the partial order, and since we are dealing with sets, we can unproblematically consider the union and intersection of upper sets. This structure is that of a lattice: 
\begin{defn}[Lattice]
A \emph{lattice} $L$ is a partially ordered set if for every two elements $a,b\in L$, there exists a least common upper bound, called a \emph{join}, denoted $a \vee b$ and a greatest common lower bound, called a \emph{meet}, denoted $a\wedge b$. Further, we require that if $a_1\leq a_2$ and $b_1\leq b_2$, then
$$a_1\wedge b_1\leq a_2\wedge b_2$$
and
$$a_1\vee b_1\leq a_2\vee b_2$$
We shall call $L$ \emph{distributive}, if
$$a\wedge(b\vee c)=(a\wedge b)\vee (b\wedge c)$$
\end{defn}
We see that we can think of the set of upper sets as a lattice when equipping it with unions and intersections. This lattice is going to be the index set for our subdivisions of manifolds, and so this will give us a natural way to think of the causal structure. We introduce the notion of a filtration:
\begin{defn}[Filtration]
Let $I$ be a partially ordered set, and let $(X_i)_{i\in I}$ be a collection of topological spaces and let further $(\iota_{ij})_{j\leq i}$ be a family of continuous maps $\iota_{ij}: X_i\to X_j$ satisfying the consistency constraint
$$\iota_{ik}=\iota_{ij}\circ \iota_{jk}$$
Then $(X_i,\iota_{ij})_I$ is an \emph{inverse system}, and we will also refer to it as a \emph{filtration}. An inverse system indexed by $I'$ obtained from $I$ by reversing all relations will be called a \emph{descending filtration} on $(X_i)_I$.
\end{defn}
This allows us to consider the causal structure as an object, and we can now use this object to create the correspondence discussed above. We will now define a descending filtration on the phase space. For this, we shall utilize that we can think of any compact finite dimensional manifold as a subspace of $\mR^n$ for suitable $n$, in fact this very fact is implied by Takens' theorem, and we shall think of the observed values as the coordinates of $\mR^n$. This reduces the generality of the approach somewhat, but it lends itself to interpretability and the modelling framework introduced in the beginning of this chapter. This leads us to the definition of a descending filtration of the phase space:
\begin{defn}[Upper set filtration]
Let $UL_\varphi$ be the lattice imposed by considering the upper sets of the component graph of a dynamical system $\varphi$. For $U\in UL_\varphi$, we define $|U|$ as the set of all variables contained the upper set $U$ and we will call it \textit{variable set} of $U$. Further, we let $\mR_U$ be the space spanned by $|U|$ of dimension $\#|U|$ and we call this the \textit{phase space} of $U$. We see now that for $U,V\in UL_\varphi$, we have $U\leq V$ if and only if $|U|\subset |V|$, which in turn is equivalent with $\mR_U\subset \mR_V$. We thus get a natural projection $\pi_{VU}:\mR_V\to\mR_U$. We further define $\pi_U$ as the natural projection $\mR^n\to \mR_U$. We see that this defines a descending filtration, and we call $(\mR_U,\pi_{UV})_{UL_\varphi}$ the \emph{upper set filtration of the phase space}.
\end{defn} 
\begin{prop}
Given a causal (\textit{definition!!}) dynamical system $\varphi$, and upper sets $U,V\in UL_\varphi$ with $U\leq V$, the family of maps $\{\varphi^t_W\}_{W\in UL_\varphi}$, where $\varphi_U^t$ is defined on $\mR_U$ and given by the equation
$$\varphi^t_U\circ\pi_U(x)=\pi_U\circ \varphi^t(x)$$
for all $x\in \mR$, is well-defined, and the the collection $(\mR_U,\pi_{UV})_{UL_\varphi}$ of dynamical systems is consistent, meaning that they satisfy
$$\pi_{VU}\circ \varphi_V^t(x)=\varphi_U^t(x)\circ\pi_{VU}(x)$$
for all $x\in \mR_V$.
\end{prop}
\begin{proof}
Suppose $\pi_U(x)=\pi_U(y)$ for $x,y\in \mR^n$. Hence $x,y$ differ in the variables $|\mathcal{V}_\varphi|\setminus |U|$, and since $U$ is an upper set, it follows that $\pi_U(\phi^t(x))=\pi_U(\phi^t(y))$, since the values of $\varphi^t$ on the variable set $|U|$ does not depend on the variables  among $|\mathcal{V}_\varphi|\setminus |U|$.\\
Observe that since each $x\in \mR_V$ can be written as $\pi_V(x')$ for some $x'\in \mR$, and since we can rewrite
\begin{align*}
\pi_U\circ\varphi^t(x')
&=\varphi_U^t(x)\circ\pi_U(x')=
\varphi_U^t\circ\pi_{VU}\circ\pi_V(x')=\varphi_U^t\circ\pi_VU(x),\quad\quad \text{and}\\
\pi_U\circ\varphi^t(x')&=\pi_{VU}\circ \pi_V\circ\varphi^t(x')=\pi_{VU}\circ \varphi_V^t\circ \pi_V(x')=\pi_VU\circ\varphi_V^t(x)
\end{align*}
by consistency of the upper set filtration, we get consistency of the collection as desired.
\end{proof}
This allows us to introduce a filtration of the manifolds on which our models live:
\begin{defn}
Let $\varphi$ be a dynamical system restricted to a compact invariant manifold. Then we will call the descending filtration $\{M_U\}_{U\in UL_\varphi}$, with $M_U$ defined by $M_U\coloneqq \pi_U(M)$, equipped with the family of maps $\pi_{VU}$, for the \emph{invariant set filtration} $\mathbb{M}$.
\end{defn}
We notice immediately that for any $U\in UL_\varphi$, then $M_U$ is a compact invariant set for the dynamical system $\varphi^t_U$. We can now decompose our dynamical systems into smaller dynamical systems based on the causal structure, and we now give a similar decomposition of the observation functions:
\begin{defn}[Filtration of observation functions]
For $U\in UL_\varphi$, let $Y_U\coloneqq C^2(M_U,\mR)$ denote the set of observation functions on $M_U$, and define $\iota_{UV}:Y_U\to Y_V$ by $\iota_UV(\varphi)=\varphi\circ \pi_{VU}:M_V\to \mR$. We call this the \emph{filtration of observation functions} and denote it $\mathbb{Y}$.
\end{defn}
To complete the journey towards the main theorem of this chapter, we need some technical results to allow us to use Takens' theorem repeatedly for each manifold in the filtration. This is not of particular interest for the
\begin{prop}
The set of generic functions is open and dense bla bla
\end{prop}
We are approaching the full-fleshed formulation of our theorem now. We can now state the assumptions in full detail and the statement that allows us to identify causal statements
\begin{thm}[Identifiability in dynamic systems]
We assume the following of our dynamic system
\begin{enumerate}[label=(I\arabic*)]
	\item The elements $M_W$ of the invariant set filtration $\mathbb{M}$ are compact, invariant \emph{smooth} manifolds. 
	\item Given comparable $U,V$ in the lattice $UL_\varphi$ with $U<V$, the corresponding projection $\pi_{VU}:M_V\to M_U$ is non-injective.
	\item Given non-comparable $U,V$ in the lattice $UL_\varphi$, there exist no continuous surjections $f:M_U\to M_V$ or $g:M_V\to M_U$.
	\item We assume that $\varphi^T$ be generic for fixed $T$, and that $s$
\end{enumerate}
Let now $U,V$ be upper sets, let $\varphi_1,\varphi_2$ be corresponding observation functions and let $M_i=\Phi_{\varphi^T,y_i,k_i}(M)$ be the reconstructions, where $k_1\geq 2n_U$ and $k_2\geq 2n_V$. It then holds that:
\begin{enumerate}[label=\roman*.]
	\item $U=V$ if and only if there exists a homeomorphism
	$$\Psi_{1,2}:M_1\to M_2$$
	\item $U<V$ if and only if there exists a continuous surjective, noninjective map
	$$\Pi_{1,2}:M_2\to M_1$$ 
\end{enumerate}
\end{thm}
We have now proven that under conditions (I1)-(I4), it is actually possible to determine the causal flow in a dynamic system. The first condition is a regularity assumption that things are sufficiently well-behaved. We will give an alternative statement, where this condition is replaced, but in spirit this type of assumption is needed for any reconstruction to take place. It is the author's view that this is a reasonable assumption in most settings: although it might never actually be true that an observed system is restricted to a compact smooth manifold, we can think as we do in any kind of modelling as the imagined dynamic being a simplified representation of the system, and as far as the observed system behaves somewhat controlled locally, such a representation will probably be useful in capturing information about the system.\\\\ The second and third conditions are somewhat less reasonable in appearance; we are assuming the non-existence of mappings that if they existed would undermine the theoretical foundation of the theorem. This is undesirable. As is done in \cite{mathFound}, this is referred to the \textit{manifold being fully resolved}. An implied interpretation hereof is that it is simply matter of observing sufficient amounts of a data to separate different parts of the system - more on this later. This assumption is quite a standard one, and it is something that will appear in practical considerations nonetheless. A further comment on this, is that it corresponds somewhat to the faithfulness assumption often made when working with structural causal models.\\\\ The final assumption is entirely impossible to test as well, but we will interpret genericity in the same way as \textit{almost everywhere}, and assume that it is unlikely that we would ever get non-generic functions by accident. The almost everywhere interpretation is often a probabilistic one whereas this one is not, but as so often the difference is philosophical.\\\\
We have to in
\begin{cor}
Let $\varphi$ be a causal dynamical system, and let $UL_\varphi$ be the corresponding lattice. Assume that assumptions I1)-I4) are fulfilled for all elements in $UL_\varphi$. Then the following algorithm recovers the transitive closure of the component graph $CG_\varphi$:
\begin{enumerate}[label=\roman*.]
	\item Let the set of variables be the set of vertices in a graph we will call \emph{RG}.
	\item For every variable $x$, build the reconstructed manifold $M_x\coloneqq \Phi	_{(\varphi^T,x,k)(M)}$.
	\item For every pair $x$ and $y$, draw an edge $x\to y$ if there exists a continuous surjection $f:M_x\to M_y$ and draw an edge $y\to x$ if $g:M_y\to M_x$.
\end{enumerate}
\end{cor}



\section{Convergent Cross Mapping and Extensions}

\section{Analysis}


\chapter{Dynamic Systems with Noise}
 



\chapter{Simulation Study}
\section{Implementation}
Takens' theorem implies that you can reconstruct the manifold by the map
$t\mapsto (y(t),y(t-\tau),...,y(t-L\tau))$
Using practical  
\section{Reference Model}
We will use the following model built on a multivariate Ornstein-Uhlenbeck process as a reference. More specifically, we will consider the following setup. Let the $d$-dimensional stochastic process $X_t$, $t\geq 0$, be given by the following stochastic differential equation
$$dX_t=M X_t\ dt+\rho I_d\ dW_t$$
with $W_t$ a $d$-dimensional  Brownian motion, $M$ a $d\times d$ drift matrix, and $\rho\in \mathbb{R}$ a scalar. We will then consider observations at time points $0\leq t_1<t_2<\cdots <t_n$, $n\in \mathbb{N}$, with some observation noise
$$Y_{t_i}=X_{t_i}+\xi_i,\quad i=1,...,n$$
where $\xi_i$ is iid with $\mathbb{E} \xi_i=0$ and $V(\xi_i)=\sigma^2$ for some variance parameter $\sigma^2\in \mathbb{R}_{\geq 0}$. Interesting variations of this model, is $\sigma^2=0$, implying no observation noise, and $\rho=0$, which removes stochasticity of the process.

\section{Simulations}
 

\chapter{Machine Learning Extensions}
\bibliographystyle{apalike2}
\bibliography{kilder}
\end{document}
