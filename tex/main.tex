\documentclass[11pt, a4paper]{memoir}
\usepackage[english, science, dropcaps, hyperref, submissionstatement]{ku-frontpage}
\usepackage{amsmath,amssymb,amsfonts,mathrsfs,latexsym,mathtools}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{ulem}
\usepackage{centernot}


\newcommand{\mN}{\mathbb{N}}
\newcommand{\mZ}{\mathbb{Z}}
\newcommand{\mQ}{\mathbb{Q}}
\newcommand{\mR}{\mathbb{R}}
\newcommand{\mD}{\mathbb{D}}
\newcommand{\mH}{\mathbb{H}}
\newcommand{\mC}{\mathbb{C}}
\newcommand{\mP}{\mathbb{P}}
\newcommand{\mF}{\mathbb{F}}
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\indep}{\perp \!\!\! \perp}

\setlength\arraycolsep{2 pt}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{0}

\assignment{Master's thesis}

% The following are only needed if the \author, \title, \subtitle, and \date
% commands are not patchable. See the readme for more information.
% \frontpageauthor{Alex Author}
% \frontpagetitle{A concise but nevertheless\\precise and interesting title}
% \frontpagesubtitle{An intruiging subtitle}
% \frontpagedate{Submitted: \today}

\frontpagetitle{Causal Inference in Dynamic Systems}
\subtitle{Convergent Cross Mapping and Alternative Approaches}
\frontpageauthor{Rasmus Juhl Christensen}
\frontpagedate{Submitted: \today}
\advisor{Advisor: Niels Richard Hansen}
\frontpageimage{example.png}

\kupdfsetup{A concise but nevertheless precise and interesting title - An intruiging subtitle}{}{Alex Author}

\begin{document}
\begingroup
  \fontencoding{T1}\fontfamily{LinuxLibertineT-OsF}\selectfont
  \maketitle
\endgroup

\tableofcontents

\section{Causal inference}
This thesis will tackle the conceptually challenging topic of causal inference. In traditional statistics, we distinguish between correlation and causation: Correlation is a feature of covariation between variables of interest, whereby one observes a linear relationship between changes in two variables and having observed a deviation in one variable makes a deviation more likely in the other. As such, this can be leveraged for prediction - you can use the one variable to better predict the other. If one has control of the one variable, say the treatment of a patient, one can randomize the treatment to ensure that if patients under treatment fare better than patients without treatment, then this can only be explained by the treatment. This allows us to infer causation, which is the feature of covariation between variables that allows one to assert that one variable causes another, which means that if you force a change in the distribution of the causing variable, you will also enforce a change in the effect variable. This is straightforward with randomization, but if you can not control your variable and only observe, then an interpretation of causation may be more tricky to defend. We will explore a framework that can allow us to infer causal relationships in dynamic systems.

\subsection{Dynamic systems}
A dynamic system is a system 

\subsection{Lnear and nonlinear systems}


\chapter{Convergent Cross Mapping}
This chapter consists of a pedagogical example, that serves to illustrate the type of problems and questions that we wish to have a framework to tackle and answer. Afterwards, mathematical definitions of the models that the framework encapture, a theoretical account of the statistical method, a reference model for testing, testing of the statistical methods and finally we return to the pedagogical example.
\section{Pedagogical example}

\section{Definitions}
In this section, we will introduce and compare definitions of causal links and we will try to illuminate the definitions and motivations of each. As mentioned before, we interpret causality as the notion that intervening in the causing variable effects an intervention in the effect variable. This can be interpreted as a distributional shift in a probabilistic setting, but it does not preclude us from using the same intuition in deterministic settings. The science of identifying such links from observational data is that of causal inference and one approach to formalization is that of the Structural Causal Model (SCM), which in its most simple form can be formulated for two variables as
\begin{align*}
C&\coloneqq N_C\\
E&\coloneqq f(C,N_E)
\end{align*}
where $N_C,N_E$ are independent random variables of some distribution and $f$ a function. In this case, we denote $C$ the cause and $E$ the effect. This simple model has some deficiencies, one being that if $N_E$ is degenerate and $f$ invertible (meaning that $x\to f(x,N_E)$ is invertible), then $E$ is the cause in the SCM
\begin{align*}
E&\coloneqq N_E\\
C&\coloneqq f_x^{-1}(E,N_C)
\end{align*}
where $N_C$ is degenerate. This prevents us from having any chance of identifying the direction of a causal link and perhaps meaningfully discussing the concept of causality. In this case, we have a deterministic relationship between our variables, and so without further structure on the problem it is perhaps most natural to consider it impossible to detect causal links in this setting. This leads us to the type of problems we consider in our setting: we introduce the dependence upon time, that is we consider a series of connected data. First of all, the most pressing problem is that of incorporating the aspect of temporal dependence in the structual causal model. The definition of the SCM easily extends to any number of variables with the definition that $X$ causes $Y$ if $Y\coloneqq f(X,Z_1,...,Z_n,N_X)$ and for a moment disregarding that $f$ may trivially depend on any variable - say in this context that $f$ depends 'non-trivially' on $X$. We can then easily imagine a a time series in an SCM in a discrete framework by simply associating each variable at a given time with a unique variable. Imposing that a variable at any given time can only cause other variables further forward in time gives this model an appropriate time structure. We inherit the problems with causality in pathological deterministic structures, and it is not immediate that this model is appropriate for continuous time models as well. However, the upside of this model is that it allows for an alternative formulation of cause and effect under some admittedly rather restrictive assumptions, namely Granger causality:\\
\textit{Consider an SCM without instantaneous effects for the time series $X_t$. Then $X^j$ causes $X^k$ if and only if there exists $t'\in \mZ$ such that
$$X_{t'}^j\centernot \indep X_{t'}^k\mid X_{t<t'}^{-j}$$}
Informally, this implies that $X^j$ causes $X^k$ if the prediction of $X^k$ based on all available information is improved when including $X^j$. In principle, this would require one to observe and include all relevant variables in the world.\\
Alternatively, we can approach the problem from a different angle. Granger causality approaches the problem from a probabilistic setting, and is thus irrelevant in deterministic settings. Furthermore, when implementing Granger causality in practice, basing causal inference upon the improvement in prediction when including a variable is heavily compromised in dynamic systems where the variables are governed by an underlying structure whereby information about other variables will be encoded in a variable simply by the nature of the dynamic system - and as such implementations of Granger causality may erroneously attribute causal links between variables that are not causally linked. In fact, in dynamic systems we have the result of Taken (1981) suggesting that observed values of one variable from a dynamic system may in fact suffice in obtaining the dynamics of the entire systems. We will discuss the theoretical details hereof later. Meanwhile we can exploit this idea of information being encoded in effect variables to great effect: this allows us to reformulate causality from the point of view of deterministic systems. The principle itself is fairly simple, but we do require some technical assumptions to make the theory work, which are not in any way empirically motivated nor entirely interpretable, but fare under what we usually consider regularity assumptions - a sweep-it-under-the-rug-interpretation would be that we just assume the data to be reasonably well-behaved. The basic idea is that if $C$ is the cause of $E$, then $E$ must contain information of $C$, since the dynamics of $C$ must be reflected in $E$ in light of a functional relationship, whereas the reverse cannot be true. Thus when reconstructing the entire dynamic system from the observed values of $E$ in accordance with the theorem, we would expect to recover the dynamics of $C$. We should however expect the reverse statement to be false, since the self-contained dynamics of $E$ would not be reflected in $C$, and so the dynamic system reconstructed from the observed values of $C$ would not recover the dynamics of $E$. Notice that this in fact implies the opposite of Granger causality: we should be able to predict the cause $C$ using the effect $E$ but not conversely. This is quite a radical difference. To obtain further understanding, we will have to delve into the theoretical details. 
\section{Theoretical details}
So the models that we will now consider are vector fields on compact manifolds. We recall the definition of a manifold as a locally Euclidean space.
Taken's theorem states the following:\\
\textit{Let M be a compact manifold of dimension $m$. If $\varphi\in \text{Diff}^2(M), y\in C^2(M,\mathbb{R})$, then the map $\Phi:M\to \mathbb{R}^{2m+1}$, defined by
$$\Phi(x)=(y(x),y(\varphi(x)),...,y(\varphi^{2m}(x)))$$
is generically an embedding.}

Let us then consider a dynamic system as a non-linear vector field $\varphi$ on a compact manifold $M$. We then consider observation functions $x,y: M\to \mathbb{R}$. Taken's theorem implies that you can reconstruct the manifold by the map
$t\mapsto (y(t),y(t-\tau),...,y(t-L\tau))$
Using practical  
\section{Reference model}
We will use the following model built on a multivariate Ornstein-Uhlenbeck process as a reference. More specifically, we will consider the following setup. Let the $d$-dimensional stochastic process $X_t$, $t\geq 0$, be given by the following stochastic differential equation
$$dX_t=M X_t\ dt+\rho I_d\ dW_t$$
with $W_t$ a $d$-dimensional  Brownian motion, $M$ a $d\times d$ drift matrix, and $\rho\in \mathbb{R}$ a scalar. We will then consider observations at time points $0\leq t_1<t_2<\cdots <t_n$, $n\in \mathbb{N}$, with some observation noise
$$Y_{t_i}=X_{t_i}+\xi_i,\quad i=1,...,n$$
where $\xi_i$ is iid with $\mathbb{E} \xi_i=0$ and $V(\xi_i)=\sigma^2$ for some variance parameter $\sigma^2\in \mathbb{R}_{\geq 0}$. Interesting variations of this model, is $\sigma^2=0$, implying no observation noise, and $\rho=0$, which removes stochasticity of the process.


\section{Simulations}
 
 
\normalem
\bibliographystyle{apalike2}
\bibliography{kilder}
\end{document}
