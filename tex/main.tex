\documentclass[11pt, a4paper]{memoir}
\usepackage[english, science, dropcaps, hyperref, submissionstatement]{ku-frontpage}
\usepackage{amsmath,amssymb,amsfonts,mathrsfs,latexsym,mathtools}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{ulem}
\usepackage{centernot}


\newcommand{\mN}{\mathbb{N}}
\newcommand{\mZ}{\mathbb{Z}}
\newcommand{\mQ}{\mathbb{Q}}
\newcommand{\mR}{\mathbb{R}}
\newcommand{\mD}{\mathbb{D}}
\newcommand{\mH}{\mathbb{H}}
\newcommand{\mC}{\mathbb{C}}
\newcommand{\mP}{\mathbb{P}}
\newcommand{\mF}{\mathbb{F}}
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\indep}{\perp \!\!\! \perp}

\setlength\arraycolsep{2 pt}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{0}

\assignment{Master's thesis}

% The following are only needed if the \author, \title, \subtitle, and \date
% commands are not patchable. See the readme for more information.
% \frontpageauthor{Alex Author}
% \frontpagetitle{A concise but nevertheless\\precise and interesting title}
% \frontpagesubtitle{An intruiging subtitle}
% \frontpagedate{Submitted: \today}

\frontpagetitle{Causal Inference in Dynamic Systems}
\subtitle{Convergent Cross Mapping and Alternative Approaches}
\frontpageauthor{Rasmus Juhl Christensen}
\frontpagedate{Submitted: \today}
\advisor{Advisor: Niels Richard Hansen}
\frontpageimage{example.png}

\kupdfsetup{A concise but nevertheless precise and interesting title - An intruiging subtitle}{}{Alex Author}

\begin{document}
\begingroup
  \fontencoding{T1}\fontfamily{LinuxLibertineT-OsF}\selectfont
  \maketitle
  \textit{Front page image generated by Chaoscope.}
\endgroup

\tableofcontents

\section{Causal inference}
This thesis will tackle the conceptually challenging topic of causal inference. In traditional statistics, we distinguish between correlation and causation: Correlation is a feature of covariation between variables of interest, whereby one observes a linear relationship between changes in two variables and having observed a deviation in one variable makes a deviation more likely in the other. As such, this can be leveraged for prediction - you can use the one variable to better predict the other. If one has control of the one variable, say the treatment of a patient, one can randomize the treatment to ensure that if patients under treatment fare better than patients without treatment, then this can only be explained by the treatment. This allows us to infer causation, which is the feature of covariation between variables that allows one to assert that one variable causes another, which means that if you force a change in the distribution of the causing variable, you will also enforce a change in the effect variable. This is straightforward with randomization, but if you can not control your variable and only observe, then an interpretation of causation may be more tricky to defend. We will explore a framework that can allow us to infer causal relationships in dynamic systems.

\subsection{Causal modelling in time series}
In this section, we will introduce and compare definitions of causal links and we will try to illuminate the definitions and motivations of each. As mentioned before, we interpret causality as the notion that intervening in the causing variable effects an intervention in the effect variable. This can be interpreted as a distributional shift in a probabilistic setting, but it does not preclude us from using the same intuition in deterministic settings. The science of identifying such links from observational data is that of causal inference and one approach to formalization is that of the Structural Causal Model (SCM), which in its most simple form can be formulated for two variables as
\begin{align*}
C&\coloneqq N_C\\
E&\coloneqq f(C,N_E)
\end{align*}
where $N_C,N_E$ are independent random variables of some distribution and $f$ a function. In this case, we denote $C$ the cause and $E$ the effect. This simple model has some deficiencies, one being that if $N_E$ is degenerate and $f$ invertible (meaning that $x\to f(x,N_E)$ is invertible), then $E$ is the cause in the SCM
\begin{align*}
E&\coloneqq N_E\\
C&\coloneqq f_x^{-1}(E,N_C)
\end{align*}
where $N_C$ is degenerate. This prevents us from having any chance of identifying the direction of a causal link and perhaps meaningfully discussing the concept of causality. In this case, we have a deterministic relationship between our variables, and so without further structure on the problem it is perhaps most natural to consider it impossible to detect causal links in this setting. This leads us to the type of problems we consider in our setting: we introduce the dependence upon time, that is we consider a series of connected data. First of all, the most pressing problem is that of incorporating the aspect of temporal dependence in the structual causal model. The definition of the SCM easily extends to any number of variables with the definition that $X$ causes $Y$ if $Y\coloneqq f(X,Z_1,...,Z_n,N_X)$ and for a moment disregarding that $f$ may trivially depend on any variable - say in this context that $f$ depends 'non-trivially' on $X$. We can then easily imagine a a time series in an SCM in a discrete framework by simply associating each variable at a given time with a unique variable. Imposing that a variable at any given time can only cause other variables further forward in time gives this model an appropriate time structure. We inherit the problems with causality in pathological deterministic structures, and it is not immediate that this model is appropriate for continuous time models as well. However, the upside of this model is that it allows for an alternative formulation of cause and effect under some admittedly rather restrictive assumptions, namely Granger causality:\\
\textit{Consider an SCM without instantaneous effects for the time series $X_t$. Then $X^j$ causes $X^k$ if and only if there exists $t'\in \mZ$ such that
$$X_{t'}^j\centernot \indep X_{t'}^k\mid X_{t<t'}^{-j}$$}
Informally, this implies that $X^j$ causes $X^k$ if the prediction of $X^k$ based on all available information is improved when including $X^j$. In principle, this would require one to observe and include all relevant variables in the world.\\
Alternatively, we can approach the problem from a different angle. Granger causality approaches the problem from a probabilistic setting, and is thus irrelevant in deterministic settings. Furthermore, when implementing Granger causality in practice, basing causal inference upon the improvement in prediction when including a variable is heavily compromised in dynamic systems where the variables are governed by an underlying structure whereby information about other variables will be encoded in a variable simply by the nature of the dynamic system - and as such implementations of Granger causality may erroneously attribute causal links between variables that are not causally linked. In fact, in dynamic systems we have the result of Takens (1981) suggesting that observed values of one variable from a dynamic system may in fact suffice in obtaining the dynamics of the entire systems. We will discuss the theoretical details hereof later. Meanwhile we can exploit this idea of information being encoded in effect variables to great effect: this allows us to reformulate causality from the point of view of deterministic systems. The principle itself is fairly simple, but we do require some technical assumptions to make the theory work, which are not in any way empirically motivated nor entirely interpretable, but fare under what we usually consider regularity assumptions - a sweep-it-under-the-rug-interpretation would be that we just assume the data to be reasonably well-behaved. The basic idea is that if $C$ is the cause of $E$, then $E$ must contain information of $C$, since the dynamics of $C$ must be reflected in $E$ in light of a functional relationship, whereas the reverse cannot be true. Thus when reconstructing the entire dynamic system from the observed values of $E$ in accordance with the theorem, we would expect to recover the dynamics of $C$. We should however expect the reverse statement to be false, since the self-contained dynamics of $E$ would not be reflected in $C$, and so the dynamic system reconstructed from the observed values of $C$ would not recover the dynamics of $E$. Notice that this in fact implies the opposite of Granger causality: we should be able to predict the cause $C$ using the effect $E$ but not conversely. This is quite a radical difference. To obtain further understanding, we will have to delve into the theoretical details. 

\subsection{Linear and nonlinear systems}


\chapter{Convergent Cross Mapping}
This chapter consists of a pedagogical example, that serves to illustrate the type of problems and questions that we wish to have a framework to tackle and answer. Afterwards, mathematical definitions of the models that the framework encapture, a theoretical account of the statistical method, a reference model for testing, testing of the statistical methods and finally we return to the pedagogical example.
\section{Pedagogical example}

\section{Definitions}

\section{Theoretical details}
In the following, we will consider different setups, but the base model is systems modelled as vector fields on invariant compact manifolds (separable, Hausdorff, locally Euclidean spaces), which can be generalized to cover vector fields with an attractor set, that need not in fact be a manifold, see \cite{Sauer1991}. The fundamental result, now known as Takens' theorem, was proved in 1981 by Floris Takens, \cite{Takens}. The original theorem as stated by Takens is as follows:\\\\
\textit{Let M be a compact manifold of dimension $m$. For pairs $(\varphi,y)\in \text{Diff}^2(M)\times C^2(M,\mathbb{R})$, it is a generic property that the map $\Phi_{(\phi,y)}:M\to \mathbb{R}^{2m+1}$, defined by
$$\Phi(x)=(y(x),y(\varphi(x)),...,y(\varphi^{2m}(x))),$$
is an embedding. \cite{Takens}}\\\\
There are several things to unpack here. First of all, this theorem is formulated in the realm of differential topology and so the objects involved in the theorem spring from there. Below we will briefly introduce and define the objects treated here. Next, there are differentiability constraints on our maps needed to make the and outline the key stages in the proof. Our goal is to be a bit more explicit with the needed assumptions, and we will in all importance be following the presentation and (re)statement of the theorem presented by \cite{Huke}. By generic property is meant to be understood that there exists an open and dense subset of the function space $C^2(M,\mR)$, respectively $\text{Diff}^2$, for which the statement is true. Now for us, what is of interest is now for what choice of observation functions $y$ this statement is true. If we can derive interpretable criteria for this property, then this can be exploited to yield the properties so desired above. Of less importance is the genericity of $\varphi$; we will usually have to assume that our choice of $\varphi$ satisfies the theorem, but we will make more explicit some theoretical considerations supporting this assumption. As for the observation function, to understand the nuance we will delve into the proof to better understand the theoretical claims. First of all, we define our objects of interest.\\\\
\textbf{Definition.} Let $M$ be a separable Hausdorff space and let $m\in \mN$. We call $(U,h)$ a \textit{chart} if $U\subset M$ is open and $h: U\to \mR^m$ is a homeomorphism onto its range with $U$ the \textit{chart domain} and $h$ the \textit{coordinate function}. If for each point $x\in M$, there exists a chart $(U,h)$ on $M$ such that $x\in U$, we call $M$ a \textit{manifold of dimension $m$}, and we call a collection of charts whose chart domains cover all of $M$ for an \textit{atlas}. The collection of all charts on $M$ is itself an atlas that we call the \textit{structure} on $M$. On overlapping chart domains of charts $(U,h)$ and $(V,g)$ we consider the \textit{coordinate transformations}
$$hg^{-1}: g(U\cap V)\to \mR^m, \text{ and } gh^{-1}: h(U\cap V)\to \mR^m$$
We say the charts are \textit{$C^r$-related} if the coordinate transformations $hg^{-1}, gh^{-1}$ are $C^r$, and if all coordinate transformations of an atlas are $C^r$-related, we say the atlas is \textit{$C^r$-differentiable}. A \textit{differential structure} is the set of all charts $C^r$-related to the charts in some atlas. We shall say a manifold is $C^r$ if there exists a $C^r$-differentiable atlas of $M$. If $M$ is $C^r$ and $N$ is a $C^r$ manifold of dimension $n\geq m$, the function $f: M\to N$ is \textit{$C^r$-differentiable} if for each $p\in M$, there exists charts $(U,h)$ on M and $(V,g)$ on N, such that $p\in U$, $f(p)\in V$ and 
$$gfh^{-1}: h(U\cap f^{-1}(V))\to \mR^n$$
is $C^r$ (remark: this property thus holds for any choice of charts in the atlas covering $p$, $f(p)$ respectively). The \textit{Jacobian} at $p$ is then defined as 
$$Dgfh^{-1}(h(p))$$
This depends on the choice of charts but the rank does not. If the Jacobian is of maximal rank (rank $m$), we say $f$ is \textit{immersive at p}, and if $f$ is immersive everywhere, then we say $f$ is an \textit{immersion}. An immersion that is homeomorphic upon its image is an \textit{embedding}. Conversely, if $m\geq n$, and the Jacobian is of maximal rank (rank $n$), then $f$ is \textit{submersive at $p$}, and if submersive everhywhere a \textit{submersion}. If $f: M\to N$ is an embedding, we say that $f(M)$ is a \textit{submanifold} of $N$. Finally, a \textit{diffeomorphism} is a map $f:M\to N$ for which there exists a differentiable inverse, and in this case we shall call $M$ and $N$ \textit{diffeomorphic}. If $f$ is an embedding, it is possible to prove that $f:M\to f(M)$ is a diffeomorphism. Finally, we will define some topologies. We let $C^r(M,N)$ denote the space of all $C^r$ differentiable maps from $M$ to $N$. We then let the topology of $C^r(M,N)$ be generated by the sets defined as follows: for any choice of function $f\in C^r(M,N)$, charts $(U,h)$ on $M$ and $(V,g)$ on $N$, a compact set $K\subset U$ such that $f(K)\subset V$ and $\varepsilon>0$, we define $\mathcal{N}(f,(U,h),(V,g),K,\varepsilon)$ to be the set of all functions $\hat{f}\in C^r(M,N)$ for which $\hat{f}(K)\subset V$ and
$$\norm{D^kg\hat{f}h^{-1}(x)-D^kgfh^{-1}(x)}<\varepsilon$$
for all $x\in h(K)$, $k=0,\ldots,r$, where $\norm{\cdot}$ is the usual Euclidean norm. If $M$ and $N$ are diffeomorphic, we let $\text{Diff}^r(M,N)$ be the subspace of $C^r(M,N)$ consisting of $C^r$-differentiable diffeomorphisms equipped with the subspace topology. Usually, we will use the shorter $\text{Diff}^r(M)$ for $\text{Diff}^r(M,M)$.\\\\
Now to the proof of Takens' theorem - first of all it is important to note that we use the subspace topology on $C^2(M,\mR)$ from $C^1(M,\mR)$ and likewise on $\text{Diff}^2(M)$. The first step is proving that the map $\mathcal{F}^2: C^2(M,\mR)\to C^2(M,\mR^{2m+1})$ defined by $y\mapsto (y,y\circ\phi,\ldots, y\circ \phi^{2m})$ is continuous. One proves $F: C^2(M,\mR)\to C^2(M,\mR)$ defined by $y\mapsto y\circ\varphi$, for $\varphi\in \text{Diff}^2(M)$, is in fact continuous.  This is simply a matter of unravelling the definition above, and requires only a slight bit of trickery. We skip the details of these derivations. By induction, it follows that $F_n:  C^2(M,\mR)\to C^2(M,\mR)$ defined by $y\mapsto y\circ\varphi^n$ is in fact continuous.  The final detail is to use that the product topology of $C^r(M,\mR)^{2m+1}$ coincides with $C^r(M,\mR^{2m+1})$. It is a standard fact that the set of functions immersive on a compact set $K\subset M$ is open in $C^2(M,\mR^{2m+1})$, respectively injective and immersive, so by continuity of $\mathcal{F}$, the inverse image is open. This proves the set of observation functions $y$ such that the delay function $\Phi = (y,y\circ\varphi, \ldots,y\circ\varphi^{2m})$ is immersive on $K$, respectively injective and immersive, is open. Taking $K=M$, we have now proven the openness part of the theorem. Worth remarking here is that we have made no assumptions on $\varphi$ yet, any embedding will do. Likewise, we have not used $m$ to do anything yet. The following stages are more interesting: to show that the set of observation functions for which the theorem is true are dense, one will have to show that for any observation function and a neighbourhood of it, there exists an observation function for which the theorem is true. \\\\
The following technique will prove fairly useful. If $y\in C^2(M,\mR)$, and $\psi_i\in C^2(M,\mR)$, $i=1,\ldots,N$ for some $N\in \mN$. Then for every neighbourhood $\mathcal{N}$ of $y$, there exists $\delta$ such that
$$y+\sum_{i=1}^N a_i\psi_i\in \mathcal{N}$$
for all $a=(a_1,\ldots,a_n)$ with $\norm{a}< \delta$. Since the set of observation functions for which the theorem is true is open, if we have any such element, then we know there exists a neighbourhood of it of such elements, and then we can construct new such elements by considering small deviations from it by the formula $y+\sum_{i=1}^N a_i\psi_i$. By iteratively exploiting this technique, we can show that any neighbourhood of an element in $M$ contains a map satisfying Takens' theorem.\\\\
Now for a more precise understanding of the mathematical foundation, we will outline a presentation very much inspired by and following quite closely \cite{mathFound}.
\section{Implementation}
Takens' theorem implies that you can reconstruct the manifold by the map
$t\mapsto (y(t),y(t-\tau),...,y(t-L\tau))$
Using practical  
\section{Reference model}
We will use the following model built on a multivariate Ornstein-Uhlenbeck process as a reference. More specifically, we will consider the following setup. Let the $d$-dimensional stochastic process $X_t$, $t\geq 0$, be given by the following stochastic differential equation
$$dX_t=M X_t\ dt+\rho I_d\ dW_t$$
with $W_t$ a $d$-dimensional  Brownian motion, $M$ a $d\times d$ drift matrix, and $\rho\in \mathbb{R}$ a scalar. We will then consider observations at time points $0\leq t_1<t_2<\cdots <t_n$, $n\in \mathbb{N}$, with some observation noise
$$Y_{t_i}=X_{t_i}+\xi_i,\quad i=1,...,n$$
where $\xi_i$ is iid with $\mathbb{E} \xi_i=0$ and $V(\xi_i)=\sigma^2$ for some variance parameter $\sigma^2\in \mathbb{R}_{\geq 0}$. Interesting variations of this model, is $\sigma^2=0$, implying no observation noise, and $\rho=0$, which removes stochasticity of the process.


\section{Simulations}
 
 
\normalem
\bibliographystyle{apalike2}
\bibliography{kilder}
\end{document}
