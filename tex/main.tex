\documentclass[11pt, a4paper]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[english, science, dropcaps, hyperref, submissionstatement]{ku-frontpage}

% Math-related packages
\usepackage{amsmath, amssymb, amsfonts, mathrsfs, latexsym, mathtools}
\usepackage{ntheorem}
\usepackage{centernot}

% Miscellaneous packages
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{ulem}
\usepackage{microtype}
\usepackage{setspace}
\usepackage{graphicx, caption}
\usepackage{cleveref}
\usepackage{chngcntr}

\DeclareCaptionStyle{mystyle}{%
  font=it, % Italif font
  labelsep=space, % Use space instead of colon
  justification=RaggedRight % Left-align the caption
}
\captionsetup{style=mystyle}
\captionsetup{belowskip=-15pt}

\crefformat{footnote}{#2\footnotemark[#1]#3}
\counterwithout{figure}{chapter}

% Theorem-like environments with italicized text
\theoremstyle{break}
\theoremheaderfont{\normalfont\bfseries}
\theorembodyfont{\itshape} % Italicized text
\theoremseparator{.}

\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}

% Definition environment with non-italicized text
\theoremstyle{break}
\theoremheaderfont{\normalfont\bfseries}
\theorembodyfont{\normalfont} % Non-italicized text
\theoremseparator{.}
\newtheorem{innerdefn}{Definition}

\newenvironment{defn}
  {\begin{innerdefn}}
  {\ensuremath{\circ}\end{innerdefn}}

\theoremstyle{nonumberplain}
\theoremsymbol{\ensuremath{\square}} 
\newtheorem{proof}{Proof}

% Custom commands and math operators
\newcommand{\mN}{\mathbb{N}}
\newcommand{\mZ}{\mathbb{Z}}
\newcommand{\mQ}{\mathbb{Q}}
\newcommand{\mR}{\mathbb{R}}
\newcommand{\mD}{\mathbb{D}}
\newcommand{\mH}{\mathbb{H}}
\newcommand{\mC}{\mathbb{C}}
\newcommand{\mP}{\mathbb{P}}
\newcommand{\mF}{\mathbb{F}}
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\indep}{\perp \!\!\! \perp}

\DeclareMathOperator{\pa}{pa}
\DeclareMathOperator{\nei}{ne}
\DeclareMathOperator{\bd}{bd}
\DeclareRobustCommand{\firstsecond}[2]{#2}

% Other settings
\setlength\arraycolsep{2 pt}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{1}
\setlength{\jot}{8pt}

\assignment{Master's thesis}

% The following are only needed if the \author, \title, \subtitle, and \date
% commands are not patchable. See the readme for more information.
% \frontpageauthor{Alex Author}
% \frontpagetitle{A concise but nevertheless\\precise and interesting title}
% \frontpagesubtitle{An intruiging subtitle}
% \frontpagedate{Submitted: \today}

\frontpagetitle{Causal Inference in Dynamic Systems}
\subtitle{Convergent Cross Mapping and Alternative Approaches}
\frontpageauthor{Rasmus Juhl Christensen}
\frontpagedate{Submitted: \today}
\advisor{Advisor: Niels Richard Hansen}
\frontpageimage{example.png}

\kupdfsetup{A concise but nevertheless precise and interesting title - An intruiging subtitle}{}{Alex Author}

\begin{document}
\begingroup
  \fontencoding{T1}\fontfamily{LinuxLibertineT-OsF}\selectfont
  \maketitle
\endgroup

\section*{Abstract}

\subsubsection*{Description from contract}
The main goal of the project is to investigate dynamic systems for time series and the effectiveness of convergent cross mapping (CCM) to detect causality in this framework compared to other approaches on the basis of a reference model. Drawing on the paper "Detecting Causality in Complex Ecosystems" (2012) and "Distinguishing time-delayed causal interactions using convergent cross mapping", the project will explore the theoretical foundations, practical applications and potential shortfalls of CCM and its link to more classical methods in causality.\\\\
Reference 1: Detecting Causality in Complex Ecosystems. George Sugihara, Robert May, Hao Ye, Chih-hao Hsieh, Ethan Deyle, Michael Fogarty and Stephan Munch. Science , 26 October 2012, New Series, Vol. 338, No. 6106 (26 October 2012), pp. 496-500.
Reference 2: Distinguishing time-delayed causal interactions using convergent cross mapping. Hao Ye, Ethan R. Deyle, Luis J. Gilarranz \& George Sugihara. Scientific Reports volume 5, Article number 14750, 2015.\\\\
 
\textit{Front page image generated by Chaoscope.}
\newpage
\tableofcontents


\vfill
\noindent All code and data used in this project is available at \url{github.com/juhlc/speciale}.

\normalem

\newpage

\section*{Introduction}
Causality is a cornerstone of understanding in various scientific domains, setting the stage for profound realizations. At its core, causal inference addresses the intricate dance between correlation and causation. While correlation hints at a mutual relationship between two variables, causation pushes us into the territory of assertion — \textit{does one variable truly influence the other?} Traditional statistics employs randomization as a useful tool. Imagine a medical scenario where you can control who receives a treatment: If you can ensure that it is perfectly random who receives the treatment and patients receiving the treatment consistently show improvement compared to those who don't, we can attribute that difference to the treatment. However, when variables are merely observed without that level of control, establishing causation becomes more complex. Causal statistics thus requires another level of statistical modelling, because we want to reason about what happens when intervening in the process.\\\\
This thesis delves into the vibrant world of causal relationships in dynamic systems. Granger causality, a seminal and widely used concept in causal analysis of time series, yields one approach to causal inference in time-dependent data. It is based on the thoughts of \cite{Granger}, and it posits that if the past of a variable $C$ contains unique information about another variable $E$, the variable $C$ is said to 'Granger-cause' the other variable $E$. This approach to causal inference is based on prediction and at its core, it operates on the principle that cause precedes effect, and therefore, knowledge of the cause can enhance the predictability of the effect. However, while Granger causality offers a framework for assessing causality in probabilistic settings, it encounters challenges in deterministic systems. Dynamic systems, by their nature, have variables that are interconnected and governed by underlying structures. This interconnectedness means that information about one variable can often be extracted from another due to the system's inherent dynamics, rather than any direct causal link.\\\\
A case in point is the result from Takens (1981), which suggests that observing a single variable from a dynamic system might be sufficient to capture the entire system's dynamics. This interconnectedness can lead Granger causality to mistakenly identify causal links where none exist. Enter now the deterministic systems' viewpoint: Instead of relying on simple prediction improvement as a marker of causality, we focus on the inherent information within variables. If variable 
$C$ causes variable $E$, then $E$ should encapsulate information about the dynamics of $C$. However, $C$ would not necessarily contain comprehensive information about $E$ because the dynamics of $E$ not imposed by $C$ would not be reflected in the values of $C$. This perspective flips Granger causality on its head: while Granger causality might predict the effect 
$C$ using the cause $E$, a deterministic viewpoint effectively suggests the opposite. \\\\
This distinction, subtle yet profound, reshapes our understanding of causality in dynamic systems. As we proceed, we will delve deeper into the theoretical underpinnings of this perspective, unpacking its implications and potential applications. In particular, this is the insight that leads to the Convergent Cross Mapping approach introduced by \cite{Sugihara}, which in some sense will be the point of departure of this thesis and will be the primary among the methods based on Takens' theorem. This approach is detailed in Chapter \ref{chapTaken}.\\\\
Yet, while the deterministic paradigm offers a fascinating lens to understand causality, there is an inherent limitation. Real-world systems, especially in fields like economics, biology, and climate science, are rife with unpredictable fluctuations. These fluctuations cannot always be attributed solely to the endogenous dynamics of the system. No matter how meticulously one captures the deterministic dynamics, there will always be elements of seeming randomness and external influences that a purely deterministic model would struggle to account for. We return thus to stochastic processes. By reintroducing a stochastic element into our models, we can more faithfully represent the unpredictability and variability inherent in real-world systems. Stochasticity provides a framework to account for the 'noise' that isn't explained by the deterministic components of a system. Moreover, it's a nod to the humility of our modeling efforts – recognizing that there are always aspects of complex systems that are beyond our current understanding or are inherently random. 
In the subsequent chapters, we will delve deeper into the fusion of deterministic dynamics with stochastic processes using  a mixture of simulation based methods and theoretical considerations seeking a harmonious balance that brings us closer to understanding the multifaceted nature of causality in dynamic systems.

\chapter{Causal Inference with State Space Reconstruction}\label{chapTaken}
In this chapter, we introduce an approach to causal inference in dynamic systems, drawing inspiration from \cite{Sugihara}. In this thesis, we will adopt a rather broad definition of dynamical systems. We will envision them as objects tracing the evolution of a set of variables of interest over time under the assumption of some deterministic and suitably regular non-linear relationship. A more detailed and precise definition will be presented in subsequent sections.\\\\ The \textit{Convergent Cross Mapping (CCM)} method of \cite{Sugihara} anchors this chapter, but we will give a broader introduction to state space reconstruction based methods given a shared theoretical foundation. The theory of state space reconstruction is formulated in the language of differential topology, and Takens' theorem of 1981, \cite{Takens}, is a cornerstone. Takens' theorem solves the problem of constructing embeddings of high-dimensional systems from a single variable under a set of fairly general assumptions. In our context, such an embedding is a \textit{state space reconstruction} of a dynamic system. We will explore this concept further within this chapter.\\\\
To illustrate the challenge of causal inference in complex dynamic systems, we will use the example of \cite{Sugihara} regarding the interactions between anchovies, sardines and sea surface temperature in the California Current ecosystem. This real-world example serves as a backdrop for the more theoretical nature of the rest of this chapter. We will also introduce the causal framework of structural causal models inspired by the account of \cite{Peters}. This framework not only offers a model for causal inference but also a guideline for formulating a causal language for models in a deterministic setting. Due to the inherent differences between the mathematical theory used to formulate models in stochastic and deterministic settings, we will aim to relate the underlying assumptions in each setting. However, we will inevitably encounter some conceptual problems, as any attempt to translate the philosophical concept of causality into mathematical terms will be endowed with non-trivial complications and notable exceptions. We will discuss these problems throughout as they arise and try to be explicit about the assumptions and potential problems of the causal framework chosen in this thesis. We will also briefly discuss alternative formulations less conducive to overly strong interpretations.\\\\
We will then turn to a more stringent introduction to the theory of embeddings. This will allow us to formulate the main theoretical results of this thesis in full detail laying bare the explicit assumptions and results concerning identifiability of causal links in our framework. We will emphasize the definitions and proof techniques used to formulate and prove Takens' theorem and discuss some expansions and alternative formulations. Using the work of \cite{mathFound}, this paves the way for the main theorem of this thesis, namely a complete characterization of the causal inference possible from state space reconstruction in dynamic systems. This will complete the introduction of the theoretical set-up of the central model of this thesis.\\\\
Concluding this chapter, we present a thorough account of CCM and its alternatives and extensions. Notably, we discuss the methods suggested by \cite{Ye2015} and \cite{Leng2020}, which address problems not resolved by CCM, respectively that of distinguishing bidirectional causality and unidirectional causality in systems with some degree of synchronicity; and that of identifying the causal chain in a system knowing the causal directions between causally related variables. We will also cover some related methods that are more rooted in machine learning but still use state space reconstruction for causal inference, albeit with less explicit theoretical foundation. We will then revisit the California Current example from the beginning and analyse it using the developed theory. However, it is crucial to remember that that the theory of this chapter applies only to purely deterministic systems, and real-world examples often have a mix of deterministic and stochastic structures. We will address this limitation in subsequent chapters, by considering a set-up replacing systems of differential equations with systems of stochastic differential equations, and analysing the effect of measurement noise or stochastic influences on the effectiveness of our methodology.
\newpage

\section{Sardines and Anchovies in the California Current System}
In this section, we will briefly discuss a real-world example concerning sardine and anchovy populations in the California Current System. This is the same example considered by \cite{Sugihara} and so we aim to use the same data for our analysis with the modification that we include data from the period 2012-2022 as well. We will use the following data:
\begin{itemize}
\item Pacific sardine (\textit{Sardinops sagax}) California fish market catch landings recorded monthly in pounds in the period January 1928 up to and including December 2022. There are 12 months with redacted data.\footnote{\label{note1}Data from January 1928 up to and including December 2002 is obtained from from \textit{ERDDAP} at \cite{oldData}, and data from January 2003 up to and including December 2022 is obtained directly from \cite{newData}.}
\item Northern anchovy (\textit{Engraulis mordax}) California fish market catch landings recorded monthly in pounds in the period January 1928 up to and including December 2022. There are 34 months with redacted data.\cref{note1} 
\item  Sea-surface temperature (SST) measured daily in $\text{C}^\circ$ as an average between temperatures measured at the two sites Scripps Pier Shore Station and Balboa Pier Shore Station in the period August 22, 1916 up to and including December 31, 2022. There are 458 days without any measurements at either site.\footnote{The data from Scripps Pier is maintained by \cite{Scripps} and covers the period from August 22, 1916 up to and including December 31, 2022. The data from Newport Pier is maintained by \cite{Newport} and covers the period November 1, 1924 up to and including December 31, 2022.} 
\end{itemize}
Commercial landings data is considered confidential and therefore some landings recordings have been redacted due to insufficient data to summarize and maintain confidentiality.\footnote{This is a consequence of the California Fish and Game Code Section 8022. See: \url{https://wildlife.ca.gov/Conservation/Marine/Data-Management-Research/MFDE/User-Guide}}. We will set landings for all months with redacted data to zero considering the cause of redaction. As for the temperature data, we impute missing data with surrounding data points considering the temperature probably does not vary significantly from day to day and that any specific day does not carry significant weight in yearly averages. Since the particular question in hand is not of significant interest for the thesis itself and only intended to be an illustration of the systems we intend to study, we shall not consider any implications of these choices in this thesis. For a comprehensive study of the interdynamics of sardine and anchovy populations, we refer to \cite{Sardine}.\\\\
We will use the landings data as a proxy for the population size of sardines and anchovies. This of course does entail some problems, since we have no measurements of the populations themselves and will result in some uncertainty, but we will disregard this discussion since it is not of inherent interest to the topic. To visualize the data, we plot the annual landings of sardine and anchovy alongside trend lines computed as the 3-year averages and we plot the 3-year averages of the SST:
\begin{center}
	\begin{figure}[!ht]
		\includegraphics[width=\textwidth]{{"../r code/sardine"}.png}
		\caption{}
	\end{figure}
\end{center}
We observe a noticeable collapse in the sardine population in the 1960's coinciding with a rapid growth of the anchovy population. In fact, there seems to be a significant negative correlation between sardine and anchovy landings in most of the 20th century. This phenomenon was not exclusive to the Califonia Current System, where the sardine ecosystem experienced an almost total ecological collapse as described in \cite{Sardine}, and it was hypothesized in the 1970's that internal competition between the two species was a driving factor in the dynamics of the system,  see for instance \cite{Compete}. In some places, the theory of interspecies competition led to suggested policies of subjecting the anchovy to heavy fishing pressure hoping that this would benefit the sardine ecosystem. Approaching the 21st century the presumed negative correlation between sardine and anchovy populations seems to disappear and and even change sign thus illustrating a feature of dynamic systems: variables can appear to be correlated, but over time this correlation may vanish or change sign. This phenomenon is named \textit{mirage correlation} by \cite{Sugihara}. This also serves as a reminder of the well-known fact that correlation does not imply causation. We observe that the SST has  risen sharply in recent years. To further illustrate the relationship between SST and the landings data, we transform the landings data to log-returns, that is the time series $(X_t)_{t\in T}\in \mR_+^T$ is transformed as follows
$$(X_t)_{t\in T} \mapsto \left(\log\left(1+\frac{X_t-X_{t-1}}{X_{t-1}}\right)\right)_{t\in T}\approx \left(\frac{X_t-X_{t-1}}{X_{t-1}}\right)_{t\in T}$$
The approximation on the right is valid when $(X_t-X_{t-1})X_{t-1}^{-1}$ is small, and thus the log-returns indicate the relative difference from year to year while shrinking very large positive values, which appear often in our data set. Below we plot the log-returns alongside 3-year averages and a scale of the 10 year average SST based on the previous 10 years for every year:
\begin{center}
	\begin{figure}[!ht]
		\includegraphics[width=\textwidth]{{"../r code/diff"}.png}
		\caption{}
	\end{figure}
\end{center}
The apparent patterns in the previous figure are significantly less obvious now. A negative correlation between sardine and anchovy populations is indeed less pronounced now, although we do see some spikes in opposite directions around 1970 and 1980. More pronounced is that the averages of the SST in the 1960's are quite low and that the return of the sardine population and decline of the anchovy population coincide with the return of higher average SST's. The climate and ecological changes connected with the development in sea surface temperatures are also deemed to be driving the population changes in the analysis done by \cite{Sardine}. We observe also a potential causal relationship between temperature and sardine and anchovy landings is \textit{regime dependent} or \textit{state dependent} thus revealing another feature of dynamic systems. In the recent warmer years, the previous dynamic between temperature and population sizes seems to disappear. This resembles the phenomenon of mirage correlation. In fact, the State of California introduced legislation in the 1990's aimed at protecting the sardine population, which implemented restrictions based on temperature. This was suspended in 2010 due to rising sea surface temperatures.\\\\
We see that the log-returns look fairly stationary over time and one approach would be to analyze this data with standard techniques for Granger causality in time series data. Let $X_t\in \mR^3$ denote the values of the log-return of sardine, the log-return of anchovy and the average of sea surface temperature at time point $t$. We then introduce the VAR($p$)-model for this system:
$$X_t=A_1 X_{t-1}+A_2 X_{t-2}+\cdots + A_p X_{t-p}+u_t$$
where $A_1,\ldots, A_p\in \mR^{3\times 3}$ coefficient matrices and $u_t$ is a noise term. The coefficient matrices can be estimated with OLS (which is also the maximum likelihood estimator when assuming that $u_t$ is normally distributed and mean-zero), and we can get standard confidence intervals based on an assumption of asymptotic normality. We see that the matrices models the influence of past values of other variables on the present values as well as the past values of the variable in question. For now, we will cautiously endow this influence with a causal interpretation, although the nuances of this interpretation is yet to be made explicit. At the very least, we will take it as an indication of a possible causal link.\\\\ 
We have to choose the parameter $p$ specifying the number of lags in the model, and this can be done by choosing the model according to a minimal information criterion like AIC. Fitting a VAR(1) model with first-differenced temperature data to the data, yields an estimate of $-2.6$ of the coefficient from temperature to anchovies and an estimate of $17.9$ of the coefficient from temperature to sardines. The remaining coefficients for sardine and anchovy are all estimated to $0.0$. No coefficient is significant 95\% level, but this is expected due to the small sample size. However, this does indicate that temperature might positively influence the sardine population and negatively influence the anchovy population. We are looking at averages of temperature over a 10-year period; any window of averaging in the range from 5 to 10 years does not change the sign and size of the estimates significantly, but when looking at the yearly averages of temperature alone, we no longer get estimates that does not round to 0. This shows that this indication is not just a happy accident, although we lack power to say anything definitively.\\\\ 
These results do not, however, exploit some of the properties of dynamic systems that we observed at discussed above, and the present approach does require some data manipulation in order to use standard econometric methods. Notice for instance, that a VAR($p$)-model under standard assumptions entail a stationary time series, and so we have to look for transformations of our time-series yielding this property or at least something that can reasonably me modelled with a stationary time series. Here this is achieved by the standard transformation, but it is a model assumption that needs addressing. This is the inspiration for the work that we will now undertake.

\section{Causal Framework}
In this section, we will introduce approaches to causal modelling and compare corresponding definitions of causation. In the process we will try to illuminate the underlying motivations. We rely on the fundamental understanding of causality as the characterization of the effect of interventions in a system of variables, and we shall build our models on the back of graphical models. In doing this, we follow the conventions of causal graphical  models as pioneered by \cite{Spirtes} and \cite{Pearl}, and of course inspired by the work of \cite{Steffen}.
\subsection{Simple Causal Model}
In that vein, one such approach to formalization is that of the \textit{Structural Causal Model (SCM)}, inspired by the terminology \cite{Peters}, which in its most simple form can be formulated for two variables as
\begin{align*}
C&\coloneqq N_C\\
E&\coloneqq f(C,N_E)
\end{align*}
where $N_C,N_E$ are independent random variables of some distribution and $f$ a suitable function. In this case, we denote $C$ the cause and $E$ the effect. We shall think of these expressions as assignments and not merely equations fulfilled by the variables of the system. By this, we mean that we think of $E$ being \textit{generated} as the functional expression of $C$ and a noise term and not that it fulfils this property by accident, from some equilibrium condition or other such imaginable circumstances. For now, this is not mathematically relevant, but it does guide how we should think of interventions. If we believe these equations to be the result of some equilibrium, then we would not necessarily expect an intervention in $C$, such as fixing its value to $C\coloneqq c$, to produce the effect that $E$ is now given by $E\coloneqq f(c,N_E)$. On the other hand, if we interpret these expressions as assignments, this should be the expectation under the model. This marks a difference to the seemingly identical \textit{Structural Equation Model (SEM)} that is favoured in the field of econometrics. In the setting of SEMs, it is less obvious how to think of interventions, but we will return to the intricacies of this distinction later on. In either case, we notice that the expressions above entail a joint distribution of $C$ and $E$. An obvious deficiency is that the representation above is not unique in the sense that varying the function and noise terms can generate models that entail the same distributions and imply the same effects of an intervention - however this can be somewhat resolved by imposing distributional constraints on the noise terms and in the way $f$ depends on the variables.\\\\ 
We are mainly interested in using these models to infer causal relationships from data, and thus this simple characterization of cause and effect is only of interest if it is actually possible to infer when observing data from the entailed joint distribution. Naturally, this is not generally feasible without further assumptions. The simplest example of this emerges when assuming $N_E$ is degenerate, i.e. $N_E\coloneqq e$ (a.s.), and $f$ is invertible in its first argument (meaning that $x\mapsto f(x,e)$ is invertible). Then $E$ is the cause in the SCM
\begin{align*}
E&\coloneqq N_E'\\
C&\coloneqq f_x^{-1}(E,N_C')
\end{align*}
with suitably chosen $N_E'$ and $N_C'$ ($N_C'$ being degenerate and $f_x^{-1}$ defined on two arguments such that for the value of $N'_C$ it yields the inverse of $x\mapsto f(x,e)$). This removes any hope of identifying the direction of a causal link and potentially obscures any meaningful attempt of a discussion of causality. Of course one could object, that this model encodes a deterministic relationship between our variables, and that without further structure it would be natural to consider the task of detecting causal links in this setting impossible. As such, this case can serve as a warning beacon for causal learning: when treating problems that feature almost deterministic relationships between variables, the causal web becomes inextricable. However, that would be quite a misleading takeaway. For one, the deterministic nature of the relationship requires more structure of the model in order to allow identification of causal links, but with the right structure we shall see that causal inference in fact does become feasible. Moreover, this problem of identifiability is more pronounced. This is quite nicely illustrated by the following result of \cite{Peters}:
\begin{thm}[Non-uniqueness of graph stuctures]
For every joint distribution\footnote{The simple proof requires the existence of conditional distributions, so we will assume this here. In fact, the theory is usually built while implicitly assuming that the distribution has a joint density.} of random variables $\mP_{(X,Y)}$, there is an SCM
$$Y=f_Y(X,N_Y),\quad X\indep N_Y$$
where $f_Y$ is a measurable function and $N_Y$ a real-valued noise variable.
\end{thm}
In fact, the construction is fairly simple: If we consider the conditional cumulative distribution function $F_{Y|x}(y)\coloneqq \mP(Y\leqslant y\mid X=x)$ and define
$$f_Y(x,n_Y)\coloneqq F^{-1}_{Y|x}(n_Y)=\inf\{y\in \mR: F_{Y|x}(y)\geqslant n_Y\}$$
then we need only let $N_Y$ be uniformly distributed on $[0,1]$ and independent of $X$ for the above SCM to entail the distribution $\mP_{(X,Y)}$. In light of this, one might feel quite pessimistic about the prospect of causal inference, but once again one has to impose the right structure on the model. If we assume that the noise or randomness is additive, then the solution  to assume sufficient 'irregularity':
\begin{thm}[Identifiability in bivariate systems with additive noise]
Assume that $(X,Y)$ has the joint distribution $\mP_{(X,Y)}$.
\begin{enumerate}[label=(\roman*)]
	\item Assume $\mP_{(X,Y)}$ admits the linear model
	$$Y=\alpha X+N_Y,\quad N_Y\indep X,$$
	with continuous random variables $X$, $N_Y$ and $Y$. Then there exists $\beta\in \mR$ and a random variable $N_X$ such that $$X=\beta Y+N_X,\quad N_X\indep Y$$
	if and only if $N_Y$ and $X$ are Gaussian. The second relies on the work of 
	\item Assume $\mP_{(X,Y)}$ admits an additive noise model
	$$Y=f_Y(X)+N_Y,\quad N_Y\indep X$$
	with continuous random variables $Y$, $X$ and $N_Y$, where $N_Y$ and $X$ have strictly positive densities $p_{N_Y}$ and $p_X$, and $f_Y$, $p_{N_Y}$ and $p_X$ are three times differentiable. Then it is a generic\footnote{In this case, this can be formulated explicitly in terms of the condition
	$$(\log p_{N_Y})''(y-f_Y(x))f_Y'(x)\neq 0$$
	for all but countably many $x$, and that we possibly discount some 3-dimensional affine subspace of log densities. In all essence, this is meant to be an indication that without any concrete evidence to the contrary, we can safely assume to be in the generic case. Later on, we will meet another case of a generic property.} property that there does not exist a measurable $f_X$ and a continuous random variable $N_X$ with strictly positive density $p_{N_X}$, such that
	$$X=f_X(Y)+N_X,\quad N_X\indep Y$$
	with $f_X$ and $p_{N_X}$ three times differentiable.
	\item Assume $\mP_{(X,Y)}$ admits an additive noise model
	$$Y=f_Y(X)+N_Y,\quad N_Y\indep X$$
	with measurable $f_Y$ and noise variable $N_Y$ and that either $X$ or $Y$ has finite support. Then there exist measurable $f_X$ and noise variable $N_X$ such that
	$$X=f_X(Y)+N_X,\quad N_X\indep Y$$
	if and only if there exists $\ell\in \mN$ and disjoint sets $C_0,\ldots, C_\ell$ satisfying
	\begin{itemize}
		\item $\bigcup_{i=0}^{\ell}C_i=\text{supp } X$
		\item $\forall i\exists d_i\geqslant 0: C_i=C_0+d_i\wedge \mP(X=x)=\mP(X=x-d_i)\cdot \frac{\mP(X\in C_i)}{\mP(X\in C_0)}$
		\item $\forall i\exists c_i: f|_{C_i}\equiv c_i\wedge \{c_i+\text{supp } N_Y\}_{i\in \{0,\ldots,\ell\}}$ is a family of disjoint sets
	\end{itemize}
	These conditions are satisfied if $X$ is uniformly distributed on $\{1,...,m\}$ for some $m\in \mN$.
\end{enumerate}
\end{thm}
The first point is a consequence of the Darmois-Skitovič theorem stating that if any two linear combinations with non-zero coefficients of a set of independent non-degenerate random variables are independent, then all the variables involved are Gaussian. The second point is the main result of \cite{ANM1}, and the third point the main result of \cite{ANM2}. In all three cases, we observe that except in rare or particularly simple examples, the direction of causality is identifiable in models with additive noise. This illustrates that with the right amount of structure and assumptions, we can render the causal direction perfectly identifiable and thus moves the problem of causal discovery into the realm of practical considerations. There exist a plethora of machine learning methods capable of reliably estimating very complex functions, as thus the question becomes one of checking an additive noise assumption, making sure that the random variables are not governed by a law exceptional to the result above and dealing with the uncertainty inherited from finite sample sizes. We refrain from entering into the practitioner's domain, and will instead use this result as an inspiration.
\subsection{Graphical Models}
We are particularly interested in modelling connected data with some sort of dependency structure, typically that of dependency of data over time. Moreover, we want to be able to include more variables in our analysis, and thus keeping track of interdependencies becomes intricate. To combat this problem, we introduce causal graphs to keep track of the causal structure of a network of variables. In this process, we will have to make some choices, that will have a significant impact on what can be expressed. We will try to address these artefacts along the way. When modelling stochastic variables with graphs, we enter the domain of graphical models, for which there exists a quite rich theory of which we will only be able to give a small taste. We refer to the thorough presentation of \cite{Steffen}, which also is the main inspiration for what is presented here. The main idea in graphical modelling is to encode (conditional) independence constraints of the system in a graph, and to use this graph as a tool for inference. This idea will be the main driver for causal inference in the purely stochastic setting as well, and therefore the models are very closely related. Emphasizing this relation serves both as a means of tracing the components of our models and as a port for importing notions from the graphical model setting where causal interpretability is more tricky. We begin with some simple graph terminology:
\begin{defn}[Graph terminology]
Consider a \emph{graph} $\mathcal{G}=(\mathcal{V},\mathcal{E})$ with $\mathcal{V}$ the set of \emph{vertices} and $\mathcal{E}\subset \mathcal{V}\times \mathcal{V}$ the set of \emph{edges}.\\[5pt]
We distinguish \textit{directed} ($-$) and \textit{undirected} edges ($\to$): For $u,v\in \mathcal{V}$, we write $u-v$ if $(u,v),(v,u)\in \mathcal{E}$, and $u\to v$ if only $(u,v)\in \mathcal{E}$.\\[5pt]
If $u\to v$, we call $u$ a \emph{parent} of $v$, and we denote by $\pa(v)$ the set of parents of $v$. If $u-v$, we call $u$ a \emph{neighbour} of $v$, and we denote by $\nei(v)$ the set of neighbours of $v$. We define the \emph{boundary} of $v$ as $\bd(v)=\pa(v)\cup \nei(v)$, and for a subset $A\subset \mathcal{V}$, we define the parents of $A$ as
$$\pa(A)=\bigcup_{v\in A}\pa(v)\setminus A $$
A \emph{path} in $\mathcal{G}$ is a sequence of vertices $v_1,\ldots, v_n$ ($n \geqslant 2$) satisfying that for all $j=1,\ldots,n-1$, we have $v_{j+1}\in \bd(v_j)$, and a graph is \emph{connected} if any two vertices are contained in a path. A \emph{directed path} is a path for which all edges between the vertices are directed. A \textit{partially directed cycle} in $\mathcal{G}$ is a cycle with at least one directed edge, that is a path $v_1,\ldots, v_n$ ($n \geqslant 3$) with $v_1\in \bd(v_n)$ and $v_{j+1}\to v_j$ for some $j$.\\[5pt]
For $A\subset V$, $\mathcal{G}_A$ denotes the subgraph of $\mathcal{G}$ with $A$ as vertex set and all edges inherited from $\mathcal{G}$, and we say it is the subgraph \emph{induced} by $A$.\\[5pt]
A \emph{chain component} is a connected  subgraph of $\mathcal{G}$ containing only undirected edges, and a \emph{minimal complex}  is an induced subgraph $\mathcal{G}_A$ with two distinguished nodes $a,b$ for which $\mathcal{G}_{A\setminus \{a,b\}}$ is a chain component and no path in $\mathcal{G}_A$ contains both $a$ and $b$. A minimal complex can be drawn as
$$a\to v_1-\cdots-v_n\gets b$$
We say that $\mathcal{G}$ is a \emph{chain graph} if it contains no partially directed cycles, and if further it contains no directed edges, we say $\mathcal{G}$ is a \emph{directed acyclic graph (DAG)}.
\end{defn}
Now we have to specify what information we want to express with a graphical representation of a set of variables. When building causal graph models, we want the graphs to encode all causal relationships between variables, while the intent behind the use of a graphical model is less ambitious from the get-go: we want 'only' to deduce conditional independences between variables from the graph. This is not to say that this is typically a weak assumption, on the contrary. Instead, it is more appropriately interpreted as a sign of just how restrictive the causal graph models are going to be of the distributions they govern. Satisfying a set of conditional independence constraints is what we shall call a Markov property of the joint distribution of a set of variables, and this is the centre of attention in graphical model theory.\\\\
For DAGs, there is a canonical way to define the Markov property, but there are multiple non-equivalent ways of defining a Markov property for chain graphs depending on how you want to treat the chain components. The choice of Markov property propagates throughout the modelling and determines what interpretations are valid. We shall encounter the possible existence of some complexes of causally interlinked variables later, and so for our purpose, we choose to define the Markov property in a fashion that treats chain components as one indivisible unit and we thereby accept the consequences for valid interpretations as discussed by \cite{ChainGraph}. These definitions are most easily expressed when assuming that there exists a joint density of the variables of interest, in which case they take the form:

\begin{defn}[Markov Property]
Let $(\Omega, \mathcal{F}, \mP)$ be a probability space. A distribution $P$ with density $f$ with respect to the product measure $\mP^{\otimes \mathcal{V}}$ is said to satisfy the \emph{Markov property with respect to the DAG} $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, if
$$f(x)=\prod_{v\in \mathcal{V}}f\left(x_v\mid x_{\pa(v)}\right)$$
where $x_A$ denotes $(x_v)_{v\in A}$ for a subset of variables $A\subset V$.\\[5pt]
A distribution $P$ with density $f$ with respect to the product measure $\mP^{\otimes \mathcal{V}}$ is said to satisfy the \emph{Markov property with respect to the chain graph} $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, if
\begin{align*}
f(x)&=\prod_{\tau\in \mathscr{T}}f\left(x_\tau\mid x_{\pa(\tau)}\right),\\
f\left(x_\tau\mid x_{\pa(\tau)}\right)&=Z^{-1}\left(x_{\pa(\tau)}\right)\prod_{A\in \mathscr{A}(\tau)}\psi_A\left(x_A\right)
\end{align*}
where $\mathscr{A}(\tau)$
\end{defn}

\subsection{Multivariate SCMs}
The definition of the SCM easily extends to any number of variables with the definition that $X$ causes $Y$ if $Y\coloneqq f(X,Z_1,...,Z_n,N_X)$ and for a moment disregarding that $f$ may trivially depend on any variable - say in this context that $f$ depends 'non-trivially' on $X$. We can then easily imagine a a time series in an SCM in a discrete framework by simply associating each variable at a given time with a unique variable. Imposing that a variable at any given time can only cause other variables further forward in time gives this model an appropriate time structure. We inherit the problems with causality in pathological deterministic structures, and it is not immediate that this model is appropriate for continuous time models as well.\\\\
We will first give an introduction to our causal modelling in the terminology of the structural causal model (SCM), sometimes also referred to as a structural equation model (SEM),  which allows us to model time series in a discrete framework. We shall then discuss the extension to continuous time series and the link to the discrete time series framework. Our causal framework does come with some caveats, but we will try our best to discuss them along the way. We will present the SCMs as done in \cite{Peters}, and we will freely use standard terminology from graph theory. We start with the central object of study.
\begin{defn}[Discrete time series]
We shall call $(X_t)_{t\in \mZ}=(X_t^{1},\ldots,X_{t}^d)_{t\in \mZ}$ a \emph{$d$-dimensional discrete time series}, where $X_t^j:\Omega\to \mR$ is a random variable and the observation of the $j$th variable of $X$ at time $t$.
\end{defn}
\begin{defn}[Causal graph]
Let $\mathcal{V}$ be a set consisting of vertices representing each observation of $X$, $X_t^j$, say $$\mathcal{V}=\left\{X_t^j: t\in \mZ,j\in\{1,\ldots,d\}\right\}\simeq \left\{(t,j)\mid t\in \mZ, j\in \{1,\ldots,d\}\right\}$$ and let $\mathcal{E}$ be a set of directed edges in the graph, that is 
$$\mathcal{E}\subset \mathcal{V}\times \mathcal{V}$$
satisfying that $(X_{t_1}^{j_1},X_{t_2}^{j_2})\in \mathcal{E}$ implies $t_1\leqslant t_2$, i.e. that there are no links backwards in time. If $t_1<t_2$ for all $(X_{t_1}^{j_1},X_{t_2}^{j_2})\in \mathcal{E}$, we shall say $\mathcal{E}$ contains no \emph{instantaneous effects}.\\
We shall call $(\mathcal{V}, \mathcal{E})$ a \emph{causal graph} of the time series $(X_t)_{t\in \mZ}$. If there exists $v_1,v_2\in \mathcal{V}$, such that $(v_1,v_2)\in \mathcal{E}$, we shall say $v_1$ is a \emph{parent} of $v_2$, write $v_1\in \text{PA}_{v_2}$, and if $v_2=X_t^j$, we shall write $\text{PA}_{t}^j$ for $\text{PA}_{v_2}$. If $X_{t_1}^{j_1}$ is the parent of $X_{t_2}^{j_2}$, we shall say $X_{t_1}^{j_1}$ \emph{directly causes} $X_{t_2}^{j_2}$ and denote $X_{t_1}^{j_1}$ the \emph{direct cause} and $X_{t_2}^{j_2}$ the \emph{direct effect}.
\end{defn}
In much of the causality literature, we would impose that the graph be acyclic, making the causal graph a directed acyclic graph (DAG). We will however concern ourselves with bidirectional causality later on, so we will not make this assumption the standard in this thesis. Remark however, that this assumption is fulfilled under any circumstance if we do not allow for instantaneous effects. With the approach of the structural causal model, we put the causal graph front and center in this modelling of causal structures.
\begin{defn}[Structural causal model]
Let $(X_t)_{t\in \mZ}$ be a discrete time series, let $\mathcal{G}=(\mathcal{V},\mathcal{E})$ be a causal graph, let $(N_v)_{v\in \mathcal{V}}$ be a family of random variables (noise variables), and let $f_1,\ldots,f_d$ be a family of deterministic functions that is not constant in any of its arguments. Then we call this setup a \emph{structural causal model}, if
$$X_t^j=f_j(\text{PA}_t^j,U_{X_{t}^j})$$
A few further assumptions are usually added here. Often, we assume the noise variables $(N_v)_{v\in \mathcal{V}}$ to satisfy that the sets $(N_{X_t^1},\ldots,N_{X_t^{d}})$ for values of $t\in \mZ$ are jointly independent. Another typical assumption is $(X_t)_{t\in \mZ}$ being strictly stationary. And finally, often we assume that $\text{PA}_t^j$ contain only observations from time period $t-q$ and forward for a fixed $q\in \mN$.\\
If for some set $A\subset V$, we replace the causal graph $G$ by a causal graph $G'$ that satisfies $\text{PA}_t^j$ is replaced by $J(j)\subset V$ for $j\in A$ in the new causal graph, and if we replace $f_j$ with some $\zeta_j$ for $j\in A$, we call the resulting new structural causal model for the \textit{postintervention structural causal model}.
\end{defn}
Mathematically, there is really nothing preventing us from defining causal graphs for continuous time series, but the interpretability of such a graph is somewhat compromised insofar that we may no longer meaningfully draw it without reverting to some discretization scheme. More problematic, is perhaps how to restrict the assignments in a meaningful fashion.
\begin{defn}[Markov property and faithfulness]
We define graph independence as following
\end{defn}

\subsection{Causal Modelling of Time Series}
We now return to the problem of incorporating the aspect of temporal dependence in the structual causal model.
The following idea was presented by Granger (reference?), and we may actually give explicit conditions for the efficacy of the Granger causality framework in our current model set-up.\\\\
%%
However, the upside of this model is that it allows for an alternative formulation of cause and effect under some admittedly rather restrictive assumptions, namely Granger causality:\\
\textit{Consider an SCM without instantaneous effects for the time series $X_t$. Then $X^j$ causes $X^k$ if and only if there exists $t'\in \mZ$ such that
$$X_{t'}^j\centernot \indep X_{t'}^k\mid X_{t<t'}^{-j}$$}
Informally, this implies that $X^j$ causes $X^k$ if the prediction of $X^k$ based on all available information is improved when including $X^j$. In principle, this would require one to observe and include all relevant variables in the world.\\
Alternatively, we can approach the problem from a different angle. Granger causality approaches the problem from a probabilistic setting, and is thus irrelevant in deterministic settings. Furthermore, when implementing Granger causality in practice, basing causal inference upon the improvement in prediction when including a variable is heavily compromised in dynamic systems where the variables are governed by an underlying structure whereby information about other variables will be encoded in a variable simply by the nature of the dynamic system - and as such implementations of Granger causality may erroneously attribute causal links between variables that are not causally linked. In fact, in dynamic systems we have the result of Takens (1981) suggesting that observed values of one variable from a dynamic system may in fact suffice in obtaining the dynamics of the entire systems. We will discuss the theoretical details hereof later. Meanwhile we can exploit this idea of information being encoded in effect variables to great effect: this allows us to reformulate causality from the point of view of deterministic systems. The principle itself is fairly simple, but we do require some technical assumptions to make the theory work, which are not in any way empirically motivated nor entirely interpretable, but fare under what we usually consider regularity assumptions - a sweep-it-under-the-rug-interpretation would be that we just assume the data to be reasonably well-behaved. The basic idea is that if $C$ is the cause of $E$, then $E$ must contain information of $C$, since the dynamics of $C$ must be reflected in $E$ in light of a functional relationship, whereas the reverse cannot be true. Thus when reconstructing the entire dynamic system from the observed values of $E$ in accordance with the theorem, we would expect to recover the dynamics of $C$. We should however expect the reverse statement to be false, since the self-contained dynamics of $E$ would not be reflected in $C$, and so the dynamic system reconstructed from the observed values of $C$ would not recover the dynamics of $E$. Notice that this in fact implies the opposite of Granger causality: we should be able to predict the cause $C$ using the effect $E$ but not conversely. This is quite a radical difference. To obtain further understanding, we will have to delve into the theoretical details. 
%%%
\begin{thm}[Granger causality]
O
\end{thm}
We now turn our attention towards continuous time models and we will present the models of driving systems as presented in \cite{mathFound}. The definition of continuous time series is straightforward a generalization of that of discrete time series.
\begin{defn}[Continuous time series]
We shall call $(X_t)_{t\in \mR}=(X_t^{1},\ldots,X_{t}^d)_{t\in \mR}$ a \emph{$d$-dimensional continuous time series}, where $X_t^j:\Omega\to \mR$ is a random variable and the observation of the $j$th variable of $X$ at time $t$.
\end{defn}
More interestingly of course is that of the dynamical systems that we will treat in this thesis. We will take it directly from \cite{mathFound}.
\begin{defn}[Dynamical system]
A \emph{dynamical system} is a collection of maps $\varphi^t:\mR^n\to\mR^n$ indexed by some $t\in \mZ$, in which case we say it is \emph{discrete}, or some $t\in \mR$, in which case we say it is \emph{continuous}.\\
We shall call a function $y:\mR^n\to \mR$ an \emph{observation function}, and we shall note that $y(\varphi^t)_t$ defines a time series.
\end{defn}
As discussed above, considering a graph on the entirety of a continuous time series.
\begin{defn}[Driver]
Given a dynamical system $\varphi$, and two variables $x$ and $y$ of $\varphi$, the variable $x$ \emph{directly drives} $y$ if the dynamics of $y$ directly depend on $x$, that is $y_{t+1}=g(x,\cdot)$ for discrete systems and $\dot{y}=g(x,\cdot)$ for continuous.
\end{defn}
\begin{defn}[Summary graph]
Given a dynamical system $\varphi$, the \emph{interaction graph} or \emph{summary graph} is a directed graph $\mathcal{IG}_\varphi=(\mathcal{V},\mathcal{E})$, with $\mathcal{V}$ corresponding to the variables of $\varphi$ and a directed edge from $x$ to $y$ if $x$ directly drives $y$.
\end{defn}
In some sense, we can think of this graph as the graph obtained by collapsing the time variable in the causal graph; if at some time point $x$ causes $y$, then we put a directed edge from $x$ to $y$ in the interaction graph. Thus we obtain a new graph containing causal directions: remark that assuming this graph is acyclic becomes more untenable.
\begin{defn}[Component graph]
A subset of vertices $W\subset V$ of the interaction graph is called a \emph{strongly connected component}, if for any $u,v\in W$ there exists a directed path from $u$ to $v$ and a directed path from $v$ to $u$. We define the \emph{component graph} as the graph obtained from the interaction graph by taking the set of strongly connected components $\mathcal{S}$ as the set of vertices, and letting there be a directed edge from $p\in \mathcal{S}$ to $q\in \mathcal{S}$ if there exists a $u\in p,v\in q$ such that there exists a directed path from $u$ to $v$.
\end{defn}
Note that being strongly connected defines an equivalence relation. The component graph will turn out to be very relevant in the following analysis.
\begin{defn}[Upper set]
An \emph{upper set} is a subset $U\subset \mathcal{S}$ satisfying that if $q\in U$ and there is a directed path from $p$ to $q$, then $p\in U$.
\end{defn}
It will turn out that each upper set corresponds to a self-contained dynamical system. The final component in our later analysis is the so-called \textit{transitive closure}:
\begin{defn}[Transitive closure]
Given a DAG $\mathcal{G}=(\mathcal{V},\mathcal{E})$, the \emph{transitive  closure of $\mathcal{G}$} is the graph obtained from $\mathcal{G}$ by taking the vertices $\mathcal{V}$ and include the edge between $u\in \mathcal{V}$ and $v\in \mathcal{V}$ if and only if there is a directed path from $u$ to $v$.
\end{defn}
The transitive closure of a causal graph encodes only the causal direction between variables but cannot distinguish between direct and indirect causal links. In other words, from the transitive closure you can infer all descendants of a given variable, but not the causal graph itself. It will turn out that this is exactly what we will be able to recover with the modelling framework based on Takens' theorem.

Now for the link between discrete and continuous time models, we will briefly present the result proved by \cite{sokol2014}, that shows that the discretization of an SDE according to the Euler scheme and treating it with the discrete time model framework will give the same results in the limit.
\section{Takens' Theorem}
In the following, we will switch gears from the more welcoming realm of considering functions in $\mR^n$ and $\mR$ and move to the realm of differential topology, which is inhabited by manifolds, vector fields and tangent bundles and worse. This is done, not for needless abstraction, but because the theory we shall be exploiting holds in this generality - and in fact as long we consider compact manifolds, we might as well think of them in Euclidean spaces nonetheless. So in practice, we will always be able to envision our models living in our usual space $\mR^n$, but it is not in any way useful to restrict ourselves at this point.\\
We will consider different setups, but the base model is systems modelled as vector fields on invariant compact manifolds (separable, Hausdorff, locally Euclidean spaces), which can be generalized to cover vector fields with an attractor set, that need not in fact be a manifold, see \cite{Sauer1991}. As the theorem itself, this is formulated in the realm of differential topology and so the objects involved in the theorem and subsequqnt theory spring from there. Below we will briefly introduce and recite the definitions of the objects of relevance, if nothing else to clarify the terminology used in this thesis. In practice, most of our examples will come from partial differential equations (PDEs) and stochastic differential equations (SDEs); we will consider the link between these realms in more detail later. We will for now interpret the manifold as the 'path' of the system, that is the values that the system takes throughout time. With terminology borrowed from physics, one could call this the \textit{phase space}. The fundamental result we shall use, now known as Takens' theorem, was proved in 1981 by Floris Takens, \cite{Takens}, and states that one can reconstruct the manifold using only a univariate observation function; that is observing only one variable over time allows us to reconstruct the entire path of the dynamic system of all variables. The original theorem as stated by Takens is as follows:
\begin{thm}[Takens, Version 1]
Let M be a compact manifold of dimension $m$. For pairs $(\varphi,y)\in \text{Diff}^2(M)\times C^2(M,\mathbb{R})$, it is a generic property that the map $\Phi_{(\varphi,y)}:M\to \mathbb{R}^{2m+1}$, defined by
$$\Phi_{(\varphi,y)}(x)=(y(x),y(\varphi(x)),...,y(\varphi^{2m}(x))),$$
is an embedding. \cite{Takens}
\end{thm}
There are several things to unpack here. First of all, it is worth noting the differentiability constraints on our maps; both $\varphi: M\to M$ and $y:M\to\mR$ are required to be twice continuously differentiable. In the work of \cite{Sauer1991}, their setup enables this requirement to be relaxed to them only being once continuously differentiable. Our dynamical system is indeed a vector field on an invariant compact manifold being the first regularity condition, this is then the second regularity condition, that we will for the most part silently have to assume is fulfilled. Next, we will call $y$ our \textit{observation function}, and we can think of this as a series of observations made from the system. For now, $\varphi$ remains a bit more mysterious, but we shall quickly make this map must more explicit. We shall call $\Phi$ for the \textit{delay embedding}, the reasoning behind which will be revealed shortly. However, the most important caveat is perhaps that it is 'only' a generic property that the theorem yields an embedding. We will provide more rigorous definitions later, but for now we will understand embedding simply as a reconstruction of the manifold. By generic property is meant to be understood that for 'most' observation functions $y$ and most choices $\varphi$, the delay embedding is indeed an embedding. The precise meaning in an algebraic sense is that there exists an open and dense subset of the function space $C^2(M,\mR)$, respectively $\text{Diff}^2$, for which the statement is true. From this stems our second regularity assumption. For $\varphi$, we will be able to do some reasoning in practical cases, but for $y$ this will often be just another silent assumption. We shall be calling a map $y\in C^2(M,\mR)$, respectively $\varphi\in \text{Diff}^2(M)$, \textit{generic} if there exists $\varphi\in \text{Diff}^2(M)$, respectively $y\in C^2(M,\mR)$, such that the resulting delay embedding is indeed an embedding.\\\\
We will now work towards a more explicit version of the theorem. First of all, the reasoning behind the naming of the delay embedding, comes from the following: we shall associate any given time point $t\in \mR_{\geqslant 0}$ with a position $x(t)\in M$ on the manifold. For a given delay length $\tau\in \mR_{>0}$, we may then define $\varphi_\tau: M\to M$ by $x(t)\mapsto x(t-\tau)$. Substituting this into Takens' theorem reduces the delay embedding to
$$\Phi(x(t))=(y(x(t)),y(x(t-\tau)),\ldots,y(x(t-2m\tau)))$$
Takens' theorem thus states that the values of a single time series considered at different delays will reconstruct the manifold 'generically'. Now we will need to ensure that this choice of $\varphi$ is both twice continuously differentiable and is not of such a nature that it falls out of the set of generic functions. The differentiability constraint falls under the umbrella of regularity assumptions, while we can guarantee that $\varphi_\tau$ will be generic for some choices of $\tau$ by the power of a restatement of Takens' theorem. It was in fact proved by Takens in \cite{Takens}, but not stated. We will follow the restatement of the theorem as well as the subsequent presentation of theory and definitions presented in \cite{Huke} for the rest of this section.
\begin{thm}[Takens, version 2]
Let M be a compact manifold of dimension $m$. Let $\varphi\in\text{Diff}^2(M)$ satisfy
\begin{enumerate}[label=\arabic*)]
	\item the periodic points of $\varphi$ with periods less than or equal to $2m$ are finite in number,
	\item if $x$ is any such periodic point with period $k\leqslant 2m$, then the eigenvalues of $\varphi^k$ at $x$ are all distinct.
\end{enumerate}
 Then it is a generic property for $y\in C^2(M,\mR)$ that the map $\Phi_{(\varphi,y)}:M\to \mathbb{R}^{2m+1}$, defined by
$$\Phi_{(\varphi,y)}(x)=(y(x),y(\varphi(x)),...,y(\varphi^{2m}(x))),$$
is an embedding. \cite{Huke}
\end{thm}
By periodic with period less than or equal to $2m$, we mean that $\varphi^{k}(x)=x$ for some $k\leqslant 2m$. If $\tau$ is chosen such that there are no periodic points of $\varphi_\tau$, or if we assume some structure on the periodic points, then we need only worry about whether $y$ is generic. In fact, if $\varphi_\tau$ is not generic for some choice of $\tau$, it will usually be after some small perturbation in $\tau$ (\textit{is it true, reference?}). Therefore in practice multiple lags can be tested, and we should thus be able to fairly easily sidestep this problem. Before moving on to definitions and technicalities, we will at this point however notice that we have a theorem that gives an explicit construction that will fairly generally allow us to completely recover the dynamics of a system from only a single variable observed over time. In contrast to the choice of $\tau$, we will however usually have in practice that the observation functions are fixed. So even if the same technique of small perturbations of a non-generic observation function produces a generic observation, we do not have access to it. Therefore we will usually have to assume a given observation function is generic, if there is no reason to believe otherwise. However, in some cases we can actually endow genericity with causal interpretation - we return to this in the next section. For now, we expand upon Takens' theorem.  We start of with the basic definitions needed following very closely the terminology and presentation from \cite{Huke2}. 
\begin{defn}[Differential topology terminology]
Let $M$ be a separable Hausdorff space and let $m\in \mN$.\\[5pt]
We call $(U,h)$ a \emph{chart} if $U\subset M$ is open and $h: U\to \mR^m$ is a homeomorphism onto its range with $U$ the \emph{chart domain} and $h$ the \emph{coordinate function}.\\[5pt]
If for each point $x\in M$, there exists a chart $(U,h)$ on $M$ such that $x\in U$, we call $M$ a \emph{manifold of dimension $m$}, and we call a collection of charts whose chart domains cover all of $M$ an \emph{atlas}.The collection of all charts on $M$ is itself an atlas that we call the \emph{structure} on $M$.\\[5pt]
On overlapping chart domains of charts $(U,h)$ and $(V,g)$, we consider the \emph{coordinate transformations}
$$hg^{-1}: g(U\cap V)\to \mR^m, \text{ and } gh^{-1}: h(U\cap V)\to \mR^m$$
We say the charts are \emph{$C^r$-related} if the coordinate transformations $hg^{-1}, gh^{-1}$ are $C^r$, and if all coordinate transformations of an atlas are $C^r$-related, we say the atlas is \emph{$C^r$-differentiable}. A \emph{differential structure} is the set of all charts $C^r$-related to the charts in some atlas. We shall say a manifold is $C^r$ if there exists a $C^r$-differentiable atlas of $M$.\\[5pt]
If $M$ is $C^r$ and $N$ is a $C^r$ manifold of dimension $n\geqslant m$, the function $f: M\to N$ is \emph{$C^r$-differentiable} if for each $p\in M$, there exists charts $(U,h)$ on M and $(V,g)$ on N, such that $p\in U$, $f(p)\in V$ and 
$$gfh^{-1}: h(U\cap f^{-1}(V))\to \mR^n$$
is $C^r$. (Remark: this property thus holds for any choice of charts in the atlas covering $p$ and $f(p)$ respectively). The \emph{Jacobian} at $p$ is then defined as 
$$Dgfh^{-1}(h(p))$$
This depends on the choice of charts but the rank does not.\\[5pt]
If the Jacobian is of maximal rank (rank $m$), we say $f$ is \emph{immersive at p}, and if $f$ is immersive everywhere, then we say $f$ is an \emph{immersion}. An immersion that is homeomorphic upon its image is an \emph{embedding}.\\[5pt]
Conversely, if $m\geqslant n$, and the Jacobian is of maximal rank (rank $n$), then $f$ is \emph{submersive at $p$}, and if submersive everywhere it is a \emph{submersion}. If $f: M\to N$ is an embedding, we say that $f(M)$ is a \emph{submanifold} of $N$.\\[5pt]
A \emph{diffeomorphism} is a map $f:M\to N$ for which there exists a differentiable inverse, and in this case we shall call $M$ and $N$ \emph{diffeomorphic}. If $f$ is an embedding, it is possible to prove that $f:M\to f(M)$ is a diffeomorphism.\\[5pt]
Finally, we will define some topologies. We let $C^r(M,N)$ denote the space of all $C^r$ differentiable maps from $M$ to $N$. We then let the topology of $C^r(M,N)$ be generated by the subbase consisting of sets defined as follows: for any choice of function $f\in C^r(M,N)$, charts $(U,h)$ on $M$ and $(V,g)$ on $N$, a compact set $K\subset U$ such that $f(K)\subset V$ and $\varepsilon>0$, we define $\mathcal{N}(f,(U,h),(V,g),K,\varepsilon)$ to be the set of all functions $\hat{f}\in C^r(M,N)$ for which $\hat{f}(K)\subset V$ and
$$\norm{D^kg\hat{f}h^{-1}(x)-D^kgfh^{-1}(x)}<\varepsilon$$
for all $x\in h(K)$, $k=0,\ldots,r$, where $\norm{\cdot}$ is the usual Euclidean norm. If $M$ and $N$ are diffeomorphic, we let $\text{Diff}^r(M,N)$ be the subspace of $C^r(M,N)$ consisting of $C^r$-differentiable diffeomorphisms equipped with the subspace topology. Usually, we will use the shorter $\text{Diff}^r(M)$ for $\text{Diff}^r(M,M)$.
\end{defn}

Before moving on, we will outline the main stages in Takens' theorem just to get an understanding of the proof techniques. In the following, we will use the subspace topology on $C^2(M,\mR)$ inherited from $C^1(M,\mR)$ and likewise on $\text{Diff}^2(M)$. In all topological statements below, this is what is meant. We proceed by proving the 2nd version of Takens' theorem as follows:
\begin{enumerate}[label=\roman*)]
	\item The first stage hangs on using the following standard fact in differential topology:
	\begin{thm}
	Let $M$ be a compact manifold, and let $N$ be any manifold. The set of $C^r$ embeddings of $M$ in $N$ is open in $C^r(M,N)$. \cite{hirsch}
	\end{thm}
	Proving that the map $\mathcal{F}^2: C^2(M,\mR)\to C^2(M,\mR^{2m+1})$, defined by $$y\mapsto (y,y\circ\varphi,\ldots, y\circ \varphi^{2m}),$$ 
	is continuous then immediately proves that the set of generic observation functions is open in $C^2(M,\mR)$. This is done in the following familiar set of steps: one proves $F: C^2(M,\mR)\to C^2(M,\mR)$ defined by $y\mapsto y\circ\varphi$, for $\varphi\in \text{Diff}^2(M)$, is in fact continuous.  This is simply a matter of unravelling the definition of the topology above, and requires only a slight bit of trickery. We skip the details of these derivations. By induction, it follows that $F_n:  C^2(M,\mR)\to C^2(M,\mR)$ defined by $y\mapsto y\circ\varphi^n$ is in fact continuous.  The final detail is to use that the product topology of $C^r(M,\mR)^{2m+1}$ coincides with that of $C^r(M,\mR^{2m+1})$. Worth remarking here is that we have made no assumptions on $\varphi$ yet, any embedding will do. Likewise, we have not used $m$ to do anything yet. 
\end{enumerate}
The following technical fact will prove fairly useful - and it is just a matter of exploiting the compactness of $M$ and using the definition of the topology. If $y\in C^2(M,\mR)$, and $\psi_i\in C^2(M,\mR)$, $i=1,\ldots,N$ for some $N\in \mN$. Then for every neighbourhood $\mathcal{N}$ of $y$, there exists $\delta$ such that
$$y+\sum_{i=1}^N a_i\psi_i\in \mathcal{N}$$
for all $a=(a_1,\ldots,a_n)$ with $\norm{a}< \delta$. 
\begin{enumerate}[label=\roman*)]
	\setcounter{enumi}{1}
	\item The following stages all revolve around exploiting this fact. To prove that the set of generic functions are dense, we consider an arbitrary $y\in C^2(M,\mR)$ and a neighbourhood $\mathcal{N}_y$ of $y$. Now we want to identify a generic observation function in $\mathcal{N}_y$. First, we return to the conditions of the 2nd version of Takens' theorem. Let us denote the points with period less than or equal to $2m$ by $P_{2m}$. Since $|P_{2m}|<\infty$, we know by the Hausdorff property of $M$, that we can find pairwise disjoint neighbourhoods of the elements in $P_{2m}$, and so by a fairly standard argument we can separate these points by observation functions, and we use the fact above. Now we have $y'\in\mathcal{N}_y$ that is injective on $P_{2m}$. Similarly, by doing some explicit calculations on the Jacobian, we can identify a function $y''$ not only injective but also immersive on $P_{2m}$. To be continued...
	\item The proof of the 1st version of Takens' theorem from the 2nd version consists of utilising the Kupka-Smale theorem, namely that it is a generic property that for $\varphi\in \text{Diff}^2(M)$ and some integer $n\in \mN$, the number of periodic points, with period $n$ or less, is finite. There is a little in work in showing that this extends to cover the second condition of the 2nd version, and one also needs to use an argument similar to that in stage i) to prove the openness claim in $\text{Diff}^2(M)\times C^2(M,\mR)$ instead of just in $C^2(M,\mR)$. But the main component is the Kupka-Smale theorem.
\end{enumerate}  
Takens' theorem will allow us to do causal inference in the modelling framework introduced in this chapter. Below we give the theoretical details of this procedure.
\section{Causal Inference with Takens' Theorem}
In this section, we will show that we can only hope to recover the component graph from data using Takens' theorem. We shall later discuss attempts to reason further about the causal structure, but the results below will show that without further assumptions, we cannot distinguish direct and indirect causal links.\\\\
The aim of this section is to show that under suitable regularity assumptions, we can create a correspondence between strongly connected components and sub-manifolds, and that this correspondence is precisely fine-grained enough to allow us to determine whether any two variables belong to the same strongly connected component and further the direction of the causal link if they do not. To do this, we have to formalize this correspondence. First of all, as discussed the component graph $CG_\varphi$ is a directed acyclic graph, and so we can think of the graph structure as a partial order, where $x,y\in CG_\varphi$ satisfy $y\geqslant x$ if $y\to x$. Considering then the upper sets, we inherit an ordering of these, since they are exactly defined with respect to the partial order, and since we are dealing with sets, we can unproblematically consider the union and intersection of upper sets. This structure is that of a lattice: 
\begin{defn}[Lattice]
A \emph{lattice} $L$ is a partially ordered set if for every two elements $a,b\in L$, there exists a least common upper bound, called a \emph{join}, denoted $a \vee b$ and a greatest common lower bound, called a \emph{meet}, denoted $a\wedge b$. Further, we require that if $a_1\leqslant a_2$ and $b_1\leqslant b_2$, then
$$a_1\wedge b_1\leqslant a_2\wedge b_2$$
and
$$a_1\vee b_1\leqslant a_2\vee b_2$$
We shall call $L$ \emph{distributive}, if
$$a\wedge(b\vee c)=(a\wedge b)\vee (b\wedge c)$$
\end{defn}
We see that we can think of the set of upper sets as a lattice when equipping it with unions and intersections. This lattice is going to be the index set for our subdivisions of manifolds, and so this will give us a natural way to think of the causal structure. We introduce the notion of a filtration:
\begin{defn}[Filtration]
Let $I$ be a partially ordered set, and let $(X_i)_{i\in I}$ be a collection of topological spaces and let further $(\iota_{ij})_{j\leqslant i}$ be a family of continuous maps $\iota_{ij}: X_i\to X_j$ satisfying the consistency constraint
$$\iota_{ik}=\iota_{ij}\circ \iota_{jk}$$
Then $(X_i,\iota_{ij})_I$ is an \emph{inverse system}, and we will also refer to it as a \emph{filtration}. An inverse system indexed by $I'$ obtained from $I$ by reversing all relations will be called a \emph{descending filtration} on $(X_i)_I$.
\end{defn}
This allows us to consider the causal structure as an object, and we can now use this object to create the correspondence discussed above. We will now define a descending filtration on the phase space. For this, we shall utilize that we can think of any compact finite dimensional manifold as a subspace of $\mR^n$ for suitable $n$, in fact this very fact is implied by Takens' theorem, and we shall think of the observed values as the coordinates of $\mR^n$. This reduces the generality of the approach somewhat, but it lends itself to interpretability and the modelling framework introduced in the beginning of this chapter. This leads us to the definition of a descending filtration of the phase space:
\begin{defn}[Upper set filtration]
Let $UL_\varphi$ be the lattice imposed by considering the upper sets of the component graph of a dynamical system $\varphi$. For $U\in UL_\varphi$, we define $|U|$ as the set of all variables contained the upper set $U$ and we will call it \textit{variable set} of $U$. Further, we let $\mR_U$ be the space spanned by $|U|$ of dimension $\#|U|$ and we call this the \textit{phase space} of $U$. We see now that for $U,V\in UL_\varphi$, we have $U\leqslant V$ if and only if $|U|\subset |V|$, which in turn is equivalent with $\mR_U\subset \mR_V$. We thus get a natural projection $\pi_{VU}:\mR_V\to\mR_U$. We further define $\pi_U$ as the natural projection $\mR^n\to \mR_U$. We see that this defines a descending filtration, and we call $(\mR_U,\pi_{UV})_{UL_\varphi}$ the \emph{upper set filtration of the phase space}.
\end{defn} 
\begin{prop}
Given a causal (\textit{definition!!}) dynamical system $\varphi$, and upper sets $U,V\in UL_\varphi$ with $U\leqslant V$, the family of maps $\{\varphi^t_W\}_{W\in UL_\varphi}$, where $\varphi_U^t$ is defined on $\mR_U$ and given by the equation
$$\varphi^t_U\circ\pi_U(x)=\pi_U\circ \varphi^t(x)$$
for all $x\in \mR$, is well-defined, and the the collection $(\mR_U,\pi_{UV})_{UL_\varphi}$ of dynamical systems is consistent, meaning that they satisfy
$$\pi_{VU}\circ \varphi_V^t(x)=\varphi_U^t(x)\circ\pi_{VU}(x)$$
for all $x\in \mR_V$.
\end{prop}
\begin{proof}
Suppose $\pi_U(x)=\pi_U(y)$ for $x,y\in \mR^n$. Hence $x,y$ differ in the variables $|\mathcal{V}_\varphi|\setminus |U|$, and since $U$ is an upper set, it follows that $\pi_U(\phi^t(x))=\pi_U(\phi^t(y))$, since the values of $\varphi^t$ on the variable set $|U|$ does not depend on the variables  among $|\mathcal{V}_\varphi|\setminus |U|$.\\
Observe that since each $x\in \mR_V$ can be written as $\pi_V(x')$ for some $x'\in \mR$, and since we can rewrite
\begin{align*}
\pi_U\circ\varphi^t(x')
&=\varphi_U^t(x)\circ\pi_U(x')=
\varphi_U^t\circ\pi_{VU}\circ\pi_V(x')=\varphi_U^t\circ\pi_VU(x),\quad\quad \text{and}\\
\pi_U\circ\varphi^t(x')&=\pi_{VU}\circ \pi_V\circ\varphi^t(x')=\pi_{VU}\circ \varphi_V^t\circ \pi_V(x')=\pi_VU\circ\varphi_V^t(x)
\end{align*}
by consistency of the upper set filtration, we get consistency of the collection as desired.
\end{proof}
This allows us to introduce a filtration of the manifolds on which our models live:
\begin{defn}
Let $\varphi$ be a dynamical system restricted to a compact invariant manifold. Then we will call the descending filtration $\{M_U\}_{U\in UL_\varphi}$, with $M_U$ defined by $M_U\coloneqq \pi_U(M)$, equipped with the family of maps $\pi_{VU}$, for the \emph{invariant set filtration} $\mathbb{M}$.
\end{defn}
We notice immediately that for any $U\in UL_\varphi$, then $M_U$ is a compact invariant set for the dynamical system $\varphi^t_U$. We can now decompose our dynamical systems into smaller dynamical systems based on the causal structure, and we now give a similar decomposition of the observation functions:
\begin{defn}[Filtration of observation functions]
For $U\in UL_\varphi$, let $Y_U\coloneqq C^2(M_U,\mR)$ denote the set of observation functions on $M_U$, and define $\iota_{UV}:Y_U\to Y_V$ by $\iota_UV(\varphi)=\varphi\circ \pi_{VU}:M_V\to \mR$. We call this the \emph{filtration of observation functions} and denote it $\mathbb{Y}$.
\end{defn}
To complete the journey towards the main theorem of this chapter, we need some technical results to allow us to use Takens' theorem repeatedly for each manifold in the filtration. This is not of particular interest for the
\begin{prop}
The set of generic functions is open and dense bla bla
\end{prop}
We are approaching the full-fleshed formulation of our theorem now. We can now state the assumptions in full detail and the statement that allows us to identify causal statements
\begin{thm}[Identifiability in dynamic systems]
We assume the following of our dynamic system
\begin{enumerate}[label=(I\arabic*)]
	\item The elements $M_W$ of the invariant set filtration $\mathbb{M}$ are compact, invariant \emph{smooth} manifolds. 
	\item Given comparable $U,V$ in the lattice $UL_\varphi$ with $U<V$, the corresponding projection $\pi_{VU}:M_V\to M_U$ is non-injective.
	\item Given non-comparable $U,V$ in the lattice $UL_\varphi$, there exist no continuous surjections $f:M_U\to M_V$ or $g:M_V\to M_U$.
	\item We assume that $\varphi^T$ be generic for fixed $T$, and that $s$
\end{enumerate}
Let now $U,V$ be upper sets, let $\varphi_1,\varphi_2$ be corresponding observation functions and let $M_i=\Phi_{\varphi^T,y_i,k_i}(M)$ be the reconstructions, where $k_1\geqslant 2n_U$ and $k_2\geqslant 2n_V$. It then holds that:
\begin{enumerate}[label=\roman*.]
	\item $U=V$ if and only if there exists a homeomorphism
	$$\Psi_{1,2}:M_1\to M_2$$
	\item $U<V$ if and only if there exists a continuous surjective, noninjective map
	$$\Pi_{1,2}:M_2\to M_1$$ 
\end{enumerate}
\end{thm}
We have now proven that under conditions (I1)-(I4), it is actually possible to determine the causal flow in a dynamic system. The first condition is a regularity assumption that things are sufficiently well-behaved. We will give an alternative statement, where this condition is replaced, but in spirit this type of assumption is needed for any reconstruction to take place. It is the author's view that this is a reasonable assumption in most settings: although it might never actually be true that an observed system is restricted to a compact smooth manifold, we can think as we do in any kind of modelling as the imagined dynamic being a simplified representation of the system, and as far as the observed system behaves somewhat controlled locally, such a representation will probably be useful in capturing information about the system.\\\\ The second and third conditions are somewhat less reasonable in appearance; we are assuming the non-existence of mappings that if they existed would undermine the theoretical foundation of the theorem. This is undesirable. As is done in \cite{mathFound}, this is referred to the \textit{manifold being fully resolved}. An implied interpretation hereof is that it is simply matter of observing sufficient amounts of a data to separate different parts of the system - more on this later. This assumption is quite a standard one, and it is something that will appear in practical considerations nonetheless. A further comment on this, is that it corresponds somewhat to the faithfulness assumption often made when working with structural causal models.\\\\ The final assumption is entirely impossible to test as well, but we will interpret genericity in the same way as \textit{almost everywhere}, and assume that it is unlikely that we would ever get non-generic functions by accident. The almost everywhere interpretation is often a probabilistic one whereas this one is not, but as so often the difference is philosophical.\\\\
We have to in
\begin{cor}
Let $\varphi$ be a causal dynamical system, and let $UL_\varphi$ be the corresponding lattice. Assume that assumptions I1)-I4) are fulfilled for all elements in $UL_\varphi$. Then the following algorithm recovers the transitive closure of the component graph $CG_\varphi$:
\begin{enumerate}[label=\roman*.]
	\item Let the set of variables be the set of vertices in a graph we will call \emph{RG}.
	\item For every variable $x$, build the reconstructed manifold $M_x\coloneqq \Phi	_{(\varphi^T,x,k)(M)}$.
	\item For every pair $x$ and $y$, draw an edge $x\to y$ if there exists a continuous surjection $f:M_x\to M_y$ and draw an edge $y\to x$ if $g:M_y\to M_x$.
\end{enumerate}
\end{cor}
We remark the inherent simplicity of this approach as compared with Granger causality: in Granger causality, we have to account for all relevant information in order to allow for causal reasoning; here we need only consider the prints of the time series themselves. Of course, some potential problems may hide in our rather abstract set of assumptions I1)-I4). For instance, we cannot rule out a causal mirage due to synchronization: imagine a common causal confounder between two systems resulting in two heavily synchronized, but essentially 'independent' dynamic systems. This would result in erroneous causal interpretations, and so the problem of hidden confounders presents itself in this world as well. However, we would expect these mirages to occur only in the presence of heavy synchronization, so this potential undermining of causal inference may be less severe than in similar problems.\\\\ 
We used the term independent before, which of course is not to be interpreted in the probabilistic sense, as this is entirely meaningless in our setup; all our objects are deterministic, and they are all trivially independent in a probabilistic model. It is not quite true in a literal way either, since we assume that they are linked by a common cause; so all  that is meant that any further dynamic not imposed by the cause must be inherent to the subsystem and thus 'independent' of other subsystems.

\section{Convergent Cross Mapping and Extensions}

\section{Analysis}


\chapter{Dynamic Systems with Noise}
 



\chapter{Simulation Study}
\section{Implementation}
Takens' theorem implies that you can reconstruct the manifold by the map
$t\mapsto (y(t),y(t-\tau),...,y(t-L\tau))$
Using practical  
\section{Reference Model}
We will use the following model built on a multivariate Ornstein-Uhlenbeck process as a reference. More specifically, we will consider the following setup. Let the $d$-dimensional stochastic process $X_t$, $t\geqslant 0$, be given by the following stochastic differential equation
$$dX_t=M X_t\ dt+\rho I_d\ dW_t$$
with $W_t$ a $d$-dimensional  Brownian motion, $M$ a $d\times d$ drift matrix, and $\rho\in \mathbb{R}$ a scalar. We will then consider observations at time points $0\leqslant t_1<t_2<\cdots <t_n$, $n\in \mathbb{N}$, with some observation noise
$$Y_{t_i}=X_{t_i}+\xi_i,\quad i=1,...,n$$
where $\xi_i$ is iid with $\mathbb{E} \xi_i=0$ and $V(\xi_i)=\sigma^2$ for some variance parameter $\sigma^2\in \mathbb{R}_{\geqslant 0}$. Interesting variations of this model, is $\sigma^2=0$, implying no observation noise, and $\rho=0$, which removes stochasticity of the process.

\section{Simulations}
 

\chapter{Machine Learning Extensions}
\bibliographystyle{apalike2}
\bibliography{kilder}
\end{document}
