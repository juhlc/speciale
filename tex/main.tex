\documentclass[11pt, a4paper]{memoir}
\usepackage[english, science, dropcaps, hyperref, submissionstatement]{ku-frontpage}
\usepackage{amsmath,amssymb,amsfonts,mathrsfs,latexsym,mathtools}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{ulem}
\usepackage{centernot}

\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}

\theoremstyle{definition}
\newtheorem{defn}{Definition}


\newcommand{\mN}{\mathbb{N}}
\newcommand{\mZ}{\mathbb{Z}}
\newcommand{\mQ}{\mathbb{Q}}
\newcommand{\mR}{\mathbb{R}}
\newcommand{\mD}{\mathbb{D}}
\newcommand{\mH}{\mathbb{H}}
\newcommand{\mC}{\mathbb{C}}
\newcommand{\mP}{\mathbb{P}}
\newcommand{\mF}{\mathbb{F}}
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\indep}{\perp \!\!\! \perp}

\setlength\arraycolsep{2 pt}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{0}

\assignment{Master's thesis}

% The following are only needed if the \author, \title, \subtitle, and \date
% commands are not patchable. See the readme for more information.
% \frontpageauthor{Alex Author}
% \frontpagetitle{A concise but nevertheless\\precise and interesting title}
% \frontpagesubtitle{An intruiging subtitle}
% \frontpagedate{Submitted: \today}

\frontpagetitle{Causal Inference in Dynamic Systems}
\subtitle{Convergent Cross Mapping and Alternative Approaches}
\frontpageauthor{Rasmus Juhl Christensen}
\frontpagedate{Submitted: \today}
\advisor{Advisor: Niels Richard Hansen}
\frontpageimage{example.png}

\kupdfsetup{A concise but nevertheless precise and interesting title - An intruiging subtitle}{}{Alex Author}

\begin{document}
\begingroup
  \fontencoding{T1}\fontfamily{LinuxLibertineT-OsF}\selectfont
  \maketitle
  \textit{Front page image generated by Chaoscope.}
\endgroup

\section{Description}
The main goal of the project is to investigate dynamic systems for time series and the effectiveness of convergent cross mapping (CCM) to detect causality in this framework compared to other approaches on the basis of a reference model. Drawing on the paper "Detecting Causality in Complex Ecosystems" (2012) and "Distinguishing time-delayed causal interactions using convergent cross mapping", the project will explore the theoretical foundations, practical applications and potential shortfalls of CCM and its link to more classical methods in causality.\\\\
Reference 1: Detecting Causality in Complex Ecosystems. George Sugihara, Robert May, Hao Ye, Chih-hao Hsieh, Ethan Deyle, Michael Fogarty and Stephan Munch. Science , 26 October 2012, New Series, Vol. 338, No. 6106 (26 October 2012), pp. 496-500.
Reference 2: Distinguishing time-delayed causal interactions using convergent cross mapping. Hao Ye, Ethan R. Deyle, Luis J. Gilarranz \& George Sugihara. Scientific Reports volume 5, Article number 14750, 2015.
\newpage

\tableofcontents

\normalem

\section{Causal inference}
This thesis will tackle the conceptually challenging topic of causal inference. In traditional statistics, we distinguish between correlation and causation: Correlation is a feature of covariation between variables of interest, whereby one observes a linear relationship between changes in two variables and having observed a deviation in one variable makes a deviation more likely in the other. As such, this can be leveraged for prediction - you can use the one variable to better predict the other. If one has control of the one variable, say the treatment of a patient, one can randomize the treatment to ensure that if patients under treatment fare better than patients without treatment, then this can only be explained by the treatment. This allows us to infer causation, which is the feature of covariation between variables that allows one to assert that one variable causes another, which means that if you force a change in the distribution of the causing variable, you will also enforce a change in the effect variable. This is straightforward with randomization, but if you can not control your variable and only observe, then an interpretation of causation may be more tricky to defend. We will explore a framework that can allow us to infer causal relationships in dynamic systems.

\subsection{Causal modelling in time series}
In this section, we will introduce and compare definitions of causal links and we will try to illuminate the definitions and motivations of each. As mentioned before, we interpret causality as the notion that intervening in the causing variable effects an intervention in the effect variable. This can be interpreted as a distributional shift in a probabilistic setting, but it does not preclude us from using the same intuition in deterministic settings. The science of identifying such links from observational data is that of causal inference and one approach to formalization is that of the Structural Causal Model (SCM), which in its most simple form can be formulated for two variables as
\begin{align*}
C&\coloneqq N_C\\
E&\coloneqq f(C,N_E)
\end{align*}
where $N_C,N_E$ are independent random variables of some distribution and $f$ a function. In this case, we denote $C$ the cause and $E$ the effect. This simple model has some deficiencies, one being that if $N_E$ is degenerate and $f$ invertible (meaning that $x\to f(x,N_E)$ is invertible), then $E$ is the cause in the SCM
\begin{align*}
E&\coloneqq N_E\\
C&\coloneqq f_x^{-1}(E,N_C)
\end{align*}
where $N_C$ is degenerate. This prevents us from having any chance of identifying the direction of a causal link and perhaps meaningfully discussing the concept of causality. In this case, we have a deterministic relationship between our variables, and so without further structure on the problem it is perhaps most natural to consider it impossible to detect causal links in this setting. This leads us to the type of problems we consider in our setting: we introduce the dependence upon time, that is we consider a series of connected data. First of all, the most pressing problem is that of incorporating the aspect of temporal dependence in the structual causal model. The definition of the SCM easily extends to any number of variables with the definition that $X$ causes $Y$ if $Y\coloneqq f(X,Z_1,...,Z_n,N_X)$ and for a moment disregarding that $f$ may trivially depend on any variable - say in this context that $f$ depends 'non-trivially' on $X$. We can then easily imagine a a time series in an SCM in a discrete framework by simply associating each variable at a given time with a unique variable. Imposing that a variable at any given time can only cause other variables further forward in time gives this model an appropriate time structure. We inherit the problems with causality in pathological deterministic structures, and it is not immediate that this model is appropriate for continuous time models as well. However, the upside of this model is that it allows for an alternative formulation of cause and effect under some admittedly rather restrictive assumptions, namely Granger causality:\\
\textit{Consider an SCM without instantaneous effects for the time series $X_t$. Then $X^j$ causes $X^k$ if and only if there exists $t'\in \mZ$ such that
$$X_{t'}^j\centernot \indep X_{t'}^k\mid X_{t<t'}^{-j}$$}
Informally, this implies that $X^j$ causes $X^k$ if the prediction of $X^k$ based on all available information is improved when including $X^j$. In principle, this would require one to observe and include all relevant variables in the world.\\
Alternatively, we can approach the problem from a different angle. Granger causality approaches the problem from a probabilistic setting, and is thus irrelevant in deterministic settings. Furthermore, when implementing Granger causality in practice, basing causal inference upon the improvement in prediction when including a variable is heavily compromised in dynamic systems where the variables are governed by an underlying structure whereby information about other variables will be encoded in a variable simply by the nature of the dynamic system - and as such implementations of Granger causality may erroneously attribute causal links between variables that are not causally linked. In fact, in dynamic systems we have the result of Takens (1981) suggesting that observed values of one variable from a dynamic system may in fact suffice in obtaining the dynamics of the entire systems. We will discuss the theoretical details hereof later. Meanwhile we can exploit this idea of information being encoded in effect variables to great effect: this allows us to reformulate causality from the point of view of deterministic systems. The principle itself is fairly simple, but we do require some technical assumptions to make the theory work, which are not in any way empirically motivated nor entirely interpretable, but fare under what we usually consider regularity assumptions - a sweep-it-under-the-rug-interpretation would be that we just assume the data to be reasonably well-behaved. The basic idea is that if $C$ is the cause of $E$, then $E$ must contain information of $C$, since the dynamics of $C$ must be reflected in $E$ in light of a functional relationship, whereas the reverse cannot be true. Thus when reconstructing the entire dynamic system from the observed values of $E$ in accordance with the theorem, we would expect to recover the dynamics of $C$. We should however expect the reverse statement to be false, since the self-contained dynamics of $E$ would not be reflected in $C$, and so the dynamic system reconstructed from the observed values of $C$ would not recover the dynamics of $E$. Notice that this in fact implies the opposite of Granger causality: we should be able to predict the cause $C$ using the effect $E$ but not conversely. This is quite a radical difference. To obtain further understanding, we will have to delve into the theoretical details. 

\subsection{Linear and nonlinear systems}


\chapter{Causal inference from state space reconstruction}
This chapter consists of a pedagogical example, that serves to illustrate the type of problems and questions that we wish to have a framework to tackle and answer. Afterwards, mathematical definitions of the models that the framework encapture, a theoretical account of the statistical method, a reference model for testing, testing of the statistical methods and finally we return to the pedagogical example.
\section{Pedagogical example}

\section{Causal model (Take 1)}
We will first give an introduction to our causal modelling in the terminology of the structural causal model (SCM), sometimes also referred to as a structural equation model (SEM),  which allows us to model time series in a discrete framework. We shall then discuss the extension to continuous time series and the link to the discrete time series framework. Our causal framework does come with some caveats, but we will try our best to discuss them along the way. We will present the SCMs as done in \cite{Peters}, and we will freely use standard terminology from graph theory. We start with the central object of study.
\begin{defn}[Discrete time series]
We shall call $(X_t)_{t\in \mZ}=(X_t^{1},\ldots,X_{t}^d)_{t\in \mZ}$ a \emph{$d$-dimensional discrete time series}, where $X_t^j:\Omega\to \mR$ is a random variable and the observation of the $j$th variable of $X$ at time $t$.
\end{defn}
\begin{defn}[Causal graph]
Let $\mathcal{V}$ be a set consisting of vertices representing each observation of $X$, $X_t^j$, say $$\mathcal{V}=\left\{X_t^j: t\in \mZ,j\in\{1,\ldots,d\}\right\}\simeq \left\{(t,j)\mid t\in \mZ, j\in \{1,\ldots,d\}\right\}$$ and let $\mathcal{E}$ be a set of directed edges in the graph, that is 
$$\mathcal{E}\subset \mathcal{V}\times \mathcal{V}$$
satisfying that $(X_{t_1}^{j_1},X_{t_2}^{j_2})\in \mathcal{E}$ implies $t_1\leq t_2$, i.e. that there are no links backwards in time. If $t_1<t_2$ for all $(X_{t_1}^{j_1},X_{t_2}^{j_2})\in \mathcal{E}$, we shall say $\mathcal{E}$ contains no \emph{instantaneous effects}.\\
We shall call $(\mathcal{V}, \mathcal{E})$ a \emph{causal graph} of the time series $(X_t)_{t\in \mZ}$. If there exists $v_1,v_2\in \mathcal{V}$, such that $(v_1,v_2)\in \mathcal{E}$, we shall say $v_1$ is a \emph{parent} of $v_2$, write $v_1\in \text{PA}_{v_2}$, and if $v_2=X_t^j$, we shall write $\text{PA}_{t}^j$ for $\text{PA}_{v_2}$. If $X_{t_1}^{j_1}$ is the parent of $X_{t_2}^{j_2}$, we shall say $X_{t_1}^{j_1}$ \emph{directly causes} $X_{t_2}^{j_2}$ and denote $X_{t_1}^{j_1}$ the \emph{direct cause} and $X_{t_2}^{j_2}$ the \emph{direct effect}.
\end{defn}
In much of the causality literature, we would impose that the graph be acyclic, making the causal graph a directed acyclic graph (DAG). We will however concern ourselves with bidirectional causality later on, so we will not make this assumption the standard in this thesis. Remark however, that this assumption is fulfilled under any circumstance if we do not allow for instantaneous effects. With the approach of the structural causal model, we put the causal graph front and center in this modelling of causal structures.
\begin{defn}[Structural causal model]
Let $(X_t)_{t\in \mZ}$ be a discrete time series, let $\mathcal{G}=(\mathcal{V},\mathcal{E})$ be a causal graph, let $(N_v)_{v\in \mathcal{V}}$ be a family of random variables (noise variables), and let $f_1,\ldots,f_d$ be a family of deterministic functions that is not constant in any of its arguments. Then we call this setup a \emph{structural causal model}, if
$$X_t^j=f_j(\text{PA}_t^j,U_{X_{t}^j})$$
A few further assumptions are usually added here. Often, we assume the noise variables $(N_v)_{v\in \mathcal{V}}$ to satisfy that the sets $(N_{X_t^1},\ldots,N_{X_t^{d}})$ for values of $t\in \mZ$ are jointly independent. Another typical assumption is $(X_t)_{t\in \mZ}$ being strictly stationary. And finally, often we assume that $\text{PA}_t^j$ contain only observations from time period $t-q$ and forward for a fixed $q\in \mN$.\\
If for some set $A\subset V$, we replace the causal graph $G$ by a causal graph $G'$ that satisfies $\text{PA}_t^j$ is replaced by $J(j)\subset V$ for $j\in A$ in the new causal graph, and if we replace $f_j$ with some $\zeta_j$ for $j\in A$, we call the resulting new structural causal model for the \textit{postintervention structural causal model}.
\end{defn}
Mathematically, there is really nothing preventing us from defining causal graphs for continuous time series, but the interpretability of such a graph is somewhat compromised insofar that we may no longer meaningfully draw it without reverting to some discretization scheme. More problematic, is perhaps how to restrict the assignments in a meaningful fashion.
\begin{defn}[Markov property and faithfulness]
We define graph independence as following
\end{defn}
The following idea was presented by Granger (reference?), and we may actually give explicit conditions for the efficacy of the Granger causality framework in our current model set-up.
\begin{thm}[Granger causality]
O
\end{thm}
We now turn our attention towards continuous time models and we will present the models of driving systems as presented in \cite{mathFound}. The definition of continuous time series is straightforward a generalization of that of discrete time series.
\begin{defn}[Continuous time series]
We shall call $(X_t)_{t\in \mR}=(X_t^{1},\ldots,X_{t}^d)_{t\in \mR}$ a \emph{$d$-dimensional continuous time series}, where $X_t^j:\Omega\to \mR$ is a random variable and the observation of the $j$th variable of $X$ at time $t$.
\end{defn}
More interestingly of course is that of the dynamical systems that we will treat in this thesis. We will take it directly from \cite{mathFound}.
\begin{defn}[Dynamical system]
A \emph{dynamical system} is a collection of maps $\varphi^t:\mR^n\to\mR^n$ indexed by some $t\in \mZ$, in which case we say it is \emph{discrete}, or some $t\in \mR$, in which case we say it is \emph{continuous}.\\
We shall call a function $y:\mR^n\to \mR$ an \emph{observation function}, and we shall note that $y(\varphi^t)_t$ defines a time series.
\end{defn}
As discussed above, considering a graph on the entirety of a continuous time series.
\begin{defn}[Driver]
Given a dynamical system $\varphi$, and two variables $x$ and $y$ of $\varphi$, the variable $x$ \emph{directly drives} $y$ if the dynamics of $y$ directly depend on $x$, that is $y_{t+1}=g(x,\cdot)$ for discrete systems and $\dot{y}=g(x,\cdot)$ for continuous.
\end{defn}
\begin{defn}[Summary graph]
Given a dynamical system $\varphi$, the \emph{interaction graph} or \emph{summary graph} is a directed graph $\mathcal{IG}_\varphi=(\mathcal{V},\mathcal{E})$, with $\mathcal{V}$ corresponding to the variables of $\varphi$ and a directed edge from $x$ to $y$ if $x$ directly drives $y$.
\end{defn}
In some sense, we can think of this graph as the graph obtained by collapsing the time variable in the causal graph; if at some time point $x$ causes $y$, then we put a directed edge from $x$ to $y$ in the interaction graph. Thus we obtain a new graph containing causal directions: remark that assuming this graph is acyclic becomes more untenable.
\begin{defn}[Component graph]
A subset of vertices $W\subset V$ of the interaction graph is called a \emph{strongly connected component}, if for any $u,v\in W$ there exists a directed path from $u$ to $v$ and a directed path from $v$ to $u$. We define the \emph{component graph} as the graph obtained from the interaction graph by taking the set of strongly connected components $\mathcal{S}$ as the set of vertices, and letting there be a directed edge from $p\in \mathcal{S}$ to $q\in \mathcal{S}$ if there exists a $u\in p,v\in q$ such that there exists a directed path from $u$ to $v$.
\end{defn}
Note that being strongly connected defines an equivalence relation. The component graph will turn out to be very relevant in the following analysis.
\begin{defn}[Upper set]
An \emph{upper set} is a subset $U\subset \mathcal{S}$ satisfying that if $p\in U$ and there is a directed path from $p$ to $q$, then $q\in U$.
\end{defn}
It will turn out that each upper set corresponds to a self-contained dynamical system.

Now for the link between discrete and continuous time models, we will briefly present the result proved by \cite{sokol2014}, that shows that the discretization of an SDE according to the Euler scheme and treating it with the discrete time model framework will give the same results in the limit.
\section{Takens' theorem}
In the following, we will switch gears from the more welcoming realm of considering functions in $\mR^n$ and $\mR$ and move to the realm of differential topology, which is inhabited by manifolds, vector fields and tangent bundles and worse. This is done, not for needless abstraction, but because the theory we shall be exploiting holds in this generality - and in fact as long we consider compact manifolds, we might as well think of them in Euclidean spaces nonetheless. So in practice, we will always be able to envision our models living in our usual space $\mR^n$, but it is not in any way useful to restrict ourselves at this point.\\
We will consider different setups, but the base model is systems modelled as vector fields on invariant compact manifolds (separable, Hausdorff, locally Euclidean spaces), which can be generalized to cover vector fields with an attractor set, that need not in fact be a manifold, see \cite{Sauer1991}. As the theorem itself, this is formulated in the realm of differential topology and so the objects involved in the theorem and subsequqnt theory spring from there. Below we will briefly introduce and recite the definitions of the objects of relevance, if nothing else to clarify the terminology used in this thesis. In practice, most of our examples will come from partial differential equations (PDEs) and stochastic differential equations (SDEs); we will consider the link between these realms in more detail later. We will for now interpret the manifold as the 'path' of the system, that is the values that the system takes throughout time. With terminology borrowed from physics, one could call this the \textit{phase space}. The fundamental result we shall use, now known as Takens' theorem, was proved in 1981 by Floris Takens, \cite{Takens}, and states that one can reconstruct the manifold using only a univariate observation function; that is observing only one variable over time allows us to reconstruct the entire path of the dynamic system of all variables. The original theorem as stated by Takens is as follows:
\begin{thm}[Takens, Version 1]
Let M be a compact manifold of dimension $m$. For pairs $(\varphi,y)\in \text{Diff}^2(M)\times C^2(M,\mathbb{R})$, it is a generic property that the map $\Phi_{(\varphi,y)}:M\to \mathbb{R}^{2m+1}$, defined by
$$\Phi_{(\varphi,y)}(x)=(y(x),y(\varphi(x)),...,y(\varphi^{2m}(x))),$$
is an embedding. \cite{Takens}
\end{thm}
There are several things to unpack here. First of all, it is worth noting the differentiability constraints on our maps; both $\varphi: M\to M$ and $y:M\to\mR$ are required to be twice continuously differentiable. In the work of \cite{Sauer1991}, their setup enables this requirement to be relaxed to them only being once continuously differentiable. Our dynamical system is indeed a vector field on an invariant compact manifold being the first regularity condition, this is then the second regularity condition, that we will for the most part silently have to assume is fulfilled. Next, we will call $y$ our \textit{observation function}, and we can think of this as a series of observations made from the system. For now, $\varphi$ remains a bit more mysterious, but we shall quickly make this map must more explicit. We shall call $\Phi$ for the \textit{delay embedding}, the reasoning behind which will be revealed shortly. However, the most important caveat is perhaps that it is 'only' a generic property that the theorem yields an embedding. We will provide more rigorous definitions later, but for now we will understand embedding simply as a reconstruction of the manifold. By generic property is meant to be understood that for 'most' observation functions $y$ and most choices $\varphi$, the delay embedding is indeed an embedding. The precise meaning in an algebraic sense is that there exists an open and dense subset of the function space $C^2(M,\mR)$, respectively $\text{Diff}^2$, for which the statement is true. From this stems our second regularity assumption. For $\varphi$, we will be able to do some reasoning in practical cases, but for $y$ this will often be just another silent assumption. We shall be calling a map $y\in C^2(M,\mR)$, respectively $\varphi\in \text{Diff}^2(M)$, \textit{generic} if there exists $\varphi\in \text{Diff}^2(M)$, respectively $y\in C^2(M,\mR)$, such that the resulting delay embedding is indeed an embedding.\\\\
We will now work towards a more explicit version of the theorem. First of all, the reasoning behind the naming of the delay embedding, comes from the following: we shall associate any given time point $t\in \mR_{\geq 0}$ with a position $x(t)\in M$ on the manifold. For a given delay length $\tau\in \mR_{>0}$, we may then define $\varphi_\tau: M\to M$ by $x(t)\mapsto x(t-\tau)$. Substituting this into Takens' theorem reduces the delay embedding to
$$\Phi(x(t))=(y(x(t)),y(x(t-\tau)),\ldots,y(x(t-2m\tau)))$$
Takens' theorem thus states that the values of a single time series considered at different delays will reconstruct the manifold 'generically'. Now we will need to ensure that this choice of $\varphi$ is both twice continuously differentiable and is not of such a nature that it falls out of the set of generic functions. The differentiability constraint falls under the umbrella of regularity assumptions, while we can guarantee that $\varphi_\tau$ will be generic for some choices of $\tau$ by the power of a restatement of Takens' theorem. It was in fact proved by Takens in \cite{Takens}, but not stated. We will follow the restatement of the theorem as well as the subsequent presentation of theory and definitions presented in \cite{Huke} for the rest of this section.
\begin{thm}[Takens, version 2]
Let M be a compact manifold of dimension $m$. Let $\varphi\in\text{Diff}^2(M)$ satisfy
\begin{enumerate}[label=\arabic*)]
	\item the periodic points of $\varphi$ with periods less than or equal to $2m$ are finite in number,
	\item if $x$ is any such periodic point with period $k\leq 2m$, then the eigenvalues of $\varphi^k$ at $x$ are all distinct.
\end{enumerate}
 Then it is a generic property for $y\in C^2(M,\mR)$ that the map $\Phi_{(\varphi,y)}:M\to \mathbb{R}^{2m+1}$, defined by
$$\Phi_{(\varphi,y)}(x)=(y(x),y(\varphi(x)),...,y(\varphi^{2m}(x))),$$
is an embedding. \cite{Huke}
\end{thm}
By periodic with period less than or equal to $2m$, we mean that $\varphi^{k}(x)=x$ for some $k\leq 2m$. If $\tau$ is chosen such that there are no periodic points of $\varphi_\tau$, or if we assume some structure on the periodic points, then we need only worry about whether $y$ is generic. In fact, if $\varphi_\tau$ is not generic for some choice of $\tau$, it will usually be after some small perturbation in $\tau$ (\textit{is it true, reference?}). Therefore in practice multiple lags can be tested, and we should thus be able to fairly easily sidestep this problem. Before moving on to definitions and technicalities, we will at this point however notice that we have a theorem that gives an explicit construction that will fairly generally allow us to completely recover the dynamics of a system from only a single variable observed over time. In contrast to the choice of $\tau$, we will however usually have in practice that the observation functions are fixed. So even if the same technique of small perturbations of a non-generic observation function produces a generic observation, we do not have access to it. Therefore we will usually have to assume a given observation function is generic, if there is no reason to believe otherwise. However, in some cases we can actually endow genericity with causal interpretation - we return to this in the next section. For now, we expand upon Takens' theorem.  We start of with the basic definitions needed following very closely the terminology and presentation from \cite{Huke2}. 
\begin{defn}[Central terms]
Let $M$ be a separable Hausdorff space and let $m\in \mN$.\\
- We call $(U,h)$ a \emph{chart} if $U\subset M$ is open and $h: U\to \mR^m$ is a homeomorphism onto its range with $U$ the \emph{chart domain} and $h$ the \emph{coordinate function}.\\
- If for each point $x\in M$, there exists a chart $(U,h)$ on $M$ such that $x\in U$, we call $M$ a \emph{manifold of dimension $m$},\\
- and we call a collection of charts whose chart domains cover all of $M$ for an \emph{atlas}.\\
- The collection of all charts on $M$ is itself an atlas that we call the \emph{structure} on $M$.\\
- On overlapping chart domains of charts $(U,h)$ and $(V,g)$, we consider the \emph{coordinate transformations}
$$hg^{-1}: g(U\cap V)\to \mR^m, \text{ and } gh^{-1}: h(U\cap V)\to \mR^m$$
- We say the charts are \emph{$C^r$-related} if the coordinate transformations $hg^{-1}, gh^{-1}$ are $C^r$,\\
- and if all coordinate transformations of an atlas are $C^r$-related, we say the atlas is \emph{$C^r$-differentiable}.\\
- A \emph{differential structure} is the set of all charts $C^r$-related to the charts in some atlas.\\
- We shall say a manifold is $C^r$ if there exists a $C^r$-differentiable atlas of $M$.\\
- If $M$ is $C^r$ and $N$ is a $C^r$ manifold of dimension $n\geq m$, the function $f: M\to N$ is \emph{$C^r$-differentiable} if for each $p\in M$, there exists charts $(U,h)$ on M and $(V,g)$ on N, such that $p\in U$, $f(p)\in V$ and 
$$gfh^{-1}: h(U\cap f^{-1}(V))\to \mR^n$$
is $C^r$ (remark: this property thus holds for any choice of charts in the atlas covering $p$, $f(p)$ respectively).\\
- The \emph{Jacobian} at $p$ is then defined as 
$$Dgfh^{-1}(h(p))$$
This depends on the choice of charts but the rank does not.\\ 
- If the Jacobian is of maximal rank (rank $m$), we say $f$ is \emph{immersive at p},\\
- and if $f$ is immersive everywhere, then we say $f$ is an \emph{immersion}.\\
- An immersion that is homeomorphic upon its image is an \emph{embedding}.\\
- Conversely, if $m\geq n$, and the Jacobian is of maximal rank (rank $n$), then $f$ is \emph{submersive at $p$},\\
- and if submersive everhywhere a \emph{submersion}.\\
- If $f: M\to N$ is an embedding, we say that $f(M)$ is a \emph{submanifold} of $N$.\\
- A \emph{diffeomorphism} is a map $f:M\to N$ for which there exists a differentiable inverse, and in this case we shall call $M$ and $N$ \emph{diffeomorphic}. If $f$ is an embedding, it is possible to prove that $f:M\to f(M)$ is a diffeomorphism.\\
- Finally, we will define some topologies. We let $C^r(M,N)$ denote the space of all $C^r$ differentiable maps from $M$ to $N$. We then let the topology of $C^r(M,N)$ be generated by the subbase consisting of sets defined as follows: for any choice of function $f\in C^r(M,N)$, charts $(U,h)$ on $M$ and $(V,g)$ on $N$, a compact set $K\subset U$ such that $f(K)\subset V$ and $\varepsilon>0$, we define $\mathcal{N}(f,(U,h),(V,g),K,\varepsilon)$ to be the set of all functions $\hat{f}\in C^r(M,N)$ for which $\hat{f}(K)\subset V$ and
$$\norm{D^kg\hat{f}h^{-1}(x)-D^kgfh^{-1}(x)}<\varepsilon$$
for all $x\in h(K)$, $k=0,\ldots,r$, where $\norm{\cdot}$ is the usual Euclidean norm. If $M$ and $N$ are diffeomorphic, we let $\text{Diff}^r(M,N)$ be the subspace of $C^r(M,N)$ consisting of $C^r$-differentiable diffeomorphisms equipped with the subspace topology. Usually, we will use the shorter $\text{Diff}^r(M)$ for $\text{Diff}^r(M,M)$.
\end{defn}

Before moving on, we will outline the main stages in Takens' theorem just to get an understanding of the proof techniques. In the following, we will use the subspace topology on $C^2(M,\mR)$ inherited from $C^1(M,\mR)$ and likewise on $\text{Diff}^2(M)$. In all topological statements below, this is what is meant. We proceed by proving the 2nd version of Takens' theorem as follows:
\begin{enumerate}[label=\roman*)]
	\item The first stage hangs on using the following standard fact in differential topology:
	\begin{thm}
	Let $M$ be a compact manifold, and let $N$ be any manifold. The set of $C^r$ embeddings of $M$ in $N$ is open in $C^r(M,N)$. \cite{hirsch}
	\end{thm}
	Proving that the map $\mathcal{F}^2: C^2(M,\mR)\to C^2(M,\mR^{2m+1})$, defined by $$y\mapsto (y,y\circ\varphi,\ldots, y\circ \varphi^{2m}),$$ 
	is continuous then immediately proves that the set of generic observation functions is open in $C^2(M,\mR)$. This is done in the following familiar set of steps: one proves $F: C^2(M,\mR)\to C^2(M,\mR)$ defined by $y\mapsto y\circ\varphi$, for $\varphi\in \text{Diff}^2(M)$, is in fact continuous.  This is simply a matter of unravelling the definition of the topology above, and requires only a slight bit of trickery. We skip the details of these derivations. By induction, it follows that $F_n:  C^2(M,\mR)\to C^2(M,\mR)$ defined by $y\mapsto y\circ\varphi^n$ is in fact continuous.  The final detail is to use that the product topology of $C^r(M,\mR)^{2m+1}$ coincides with that of $C^r(M,\mR^{2m+1})$. Worth remarking here is that we have made no assumptions on $\varphi$ yet, any embedding will do. Likewise, we have not used $m$ to do anything yet. 
\end{enumerate}
The following technical fact will prove fairly useful - and it is just a matter of exploiting the compactness of $M$ and using the definition of the topology. If $y\in C^2(M,\mR)$, and $\psi_i\in C^2(M,\mR)$, $i=1,\ldots,N$ for some $N\in \mN$. Then for every neighbourhood $\mathcal{N}$ of $y$, there exists $\delta$ such that
$$y+\sum_{i=1}^N a_i\psi_i\in \mathcal{N}$$
for all $a=(a_1,\ldots,a_n)$ with $\norm{a}< \delta$. 
\begin{enumerate}[label=\roman*)]
	\setcounter{enumi}{1}
	\item The following stages all revolve around exploiting this fact. To prove that the set of generic functions are dense, we consider an arbitrary $y\in C^2(M,\mR)$ and a neighbourhood $\mathcal{N}_y$ of $y$. Now we want to identify a generic observation function in $\mathcal{N}_y$. First, we return to the conditions of the 2nd version of Takens' theorem. Let us denote the points with period less than or equal to $2m$ by $P_{2m}$. Since $|P_{2m}|<\infty$, we know by the Hausdorff property of $M$, that we can find pairwise disjoint neighbourhoods of the elements in $P_{2m}$, and so by a fairly standard argument we can separate these points by observation functions, and we use the fact above. Now we have $y'\in\mathcal{N}_y$ that is injective on $P_{2m}$. Similarly, by doing some explicit calculations on the Jacobian, we can identify a function $y''$ not only injective but also immersive on $P_{2m}$. To be continued...
	\item The proof of the 1st version of Takens' theorem from the 2nd version consists of utilising the Kupka-Smale theorem, namely that it is a generic property that for $\varphi\in \text{Diff}^2(M)$ and some integer $n\in \mN$, the number of periodic points, with period $n$ or less, is finite. There is a little in work in showing that this extends to cover the second condition of the 2nd version, and one also needs to use an argument similar to that in stage i) to prove the openness claim in $\text{Diff}^2(M)\times C^2(M,\mR)$ instead of just in $C^2(M,\mR)$. But the main component is the Kupka-Smale theorem.
\end{enumerate}  
Takens' theorem will allow us to do causal inference in the modelling framework introduced in this chapter. Below we give the theoretical details of this procedure.
\section{Causal inference with Takens' theorem}
In this section, we will show that we can only hope to recover the component graph from data using Takens' theorem. We shall later discuss attempts to reason further about the causal structure, but the results below will show that without further assumptions, we cannot distinguish direct and indirect causal links.\\\\
The aim of this section is to show that under suitable regularity assumptions, we can create a correspondence between strongly connected components and sub-manifolds, and that this correspondence is precisely fine-grained enough to allow us to determine whether any two variables belong to the same strongly connected component and further the direction of the causal link if they do not. To do this, we have to formalize this correspondence. First of all, as discussed the component graph $CG_\varphi$ is a directed acyclic graph, and so we can think of the graph structure as a partial order, where $x,y\in CG_\varphi$ satisfy $y\geq x$ if $y\to x$. Considering then the upper sets, we inherit an ordering of these, since they are exactly defined with respect to the partial order, and since we are dealing with sets, we can unproblematically consider the union and intersection of upper sets. This structure is that of a lattice: 
\begin{defn}[Lattice]
A \emph{lattice} $L$ is a partially ordered set if for every two elements $a,b\in L$, there exists a least common upper bound, called a \emph{join}, denoted $a \vee b$ and a greatest common lower bound, called a \emph{meet}, denoted $a\wedge b$. Further, we require that if $a_1\leq a_2$ and $b_1\leq b_2$, then
$$a_1\wedge b_1\leq a_2\wedge b_2$$
and
$$a_1\vee b_1\leq a_2\vee b_2$$
We shall call $L$ \emph{distributive}, if
$$a\wedge(b\vee c)=(a\wedge b)\vee (b\wedge c)$$
\end{defn}
We see that we can think of the set of upper sets as a lattice when equipping it with unions and intersections. This lattice is going to be the index set for our subdivisions of manifolds, and so this will give us a natural way to think of the causal structure. We introduce the notion of a filtration:
\begin{defn}[Filtration]
Let $I$ be a partially ordered set, and let $(X_i)_{i\in I}$ be a collection of topological spaces and let further $(\iota_{ij})_{j\leq i}$ be a family of continuous maps $\iota_{ij}: X_i\to X_j$ satisfying the consistency constraint
$$\iota_{ik}=\iota_{ij}\circ \iota_{jk}$$
Then $(X_i,\iota_{ij})_I$ is an \emph{inverse system}, and we will also refer to it as a \emph{filtration}. An inverse system indexed by $I'$ obtained by reversing all relations will be called a \emph{descending filtration} on $(X_i)_I$.
\end{defn}


\section{Applications}

\section{Dynamic systems with noise}
 



\chapter{Simulation study}
\section{Implementation}
Takens' theorem implies that you can reconstruct the manifold by the map
$t\mapsto (y(t),y(t-\tau),...,y(t-L\tau))$
Using practical  
\section{Reference model}
We will use the following model built on a multivariate Ornstein-Uhlenbeck process as a reference. More specifically, we will consider the following setup. Let the $d$-dimensional stochastic process $X_t$, $t\geq 0$, be given by the following stochastic differential equation
$$dX_t=M X_t\ dt+\rho I_d\ dW_t$$
with $W_t$ a $d$-dimensional  Brownian motion, $M$ a $d\times d$ drift matrix, and $\rho\in \mathbb{R}$ a scalar. We will then consider observations at time points $0\leq t_1<t_2<\cdots <t_n$, $n\in \mathbb{N}$, with some observation noise
$$Y_{t_i}=X_{t_i}+\xi_i,\quad i=1,...,n$$
where $\xi_i$ is iid with $\mathbb{E} \xi_i=0$ and $V(\xi_i)=\sigma^2$ for some variance parameter $\sigma^2\in \mathbb{R}_{\geq 0}$. Interesting variations of this model, is $\sigma^2=0$, implying no observation noise, and $\rho=0$, which removes stochasticity of the process.


\section{Simulations}
 

\chapter{Machine learning}
\bibliographystyle{apalike2}
\bibliography{kilder}
\end{document}
